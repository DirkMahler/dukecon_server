{
  "okay": true,
  "rooms": [{
    "id": "17989985db1fe0ab8",
    "name": "Santa Cruz",
    "days": [{
      "day": 1479340800,
      "slots": [{
        "starttime": 1479380400,
        "origin": "17989985db1fe0ab8",
        "id": "7d91cea14f32bc1d8",
        "duration": 50,
        "readonly": false,
        "request_id": "7d91cea14f32bc1d8",
        "talk": {
          "description": "This talk will be about the integration of Apache Samoa, a distributed streaming machine learning framework (https:\/\/samoa.incubator.apache.org) with Apache Apex, a distributed, scalable and fault-tolerant stream processing engine (https:\/\/apex.apache.org). Apache Samoa is a kind of WORA (write-once-run-anywhere) framework where algorithms developed on Samoa can be run on other distributed stream processing engines like Storm, Samza and Flink. This talk will introduce the integration story with Apache Apex and outline the process and the challenges therein. In addition, the talk will also dwell upon some comparative analysis on the performance of Samoa algorithms on few popular integrated runners, namely Apache Storm, Apache Flink and Apache Apex. ",
          "onhold": false,
          "bio": "Bhupesh Chawda is a Software Engineer at DataTorrent Software, India and a committer on the Apache Apex project under the Apache Software Foundation. Previously he was a Research Engineer at IBM India Research Labs, New Delhi. His interests are in the areas of distributed systems, stream processing and machine learning. He has experience delivering talks at international conferences like EDBT (2013) and ACM IKDD CODS (2016). He has publications in top tier international venues such as the VLDB journal, ICDE, EDBT, DBTest (SIGMOD), IJCAI and WISE. Bhupesh holds an M.Tech. in Computer Science and Engineering from IIT Bombay, India (2011) and a BE in Computer Engineering from the University of Pune, India (2007).",
          "request_id": "30d56ffb022f8f692f0632f",
          "category": "ML",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.9579,
          "notes": "",
          "tags": "",
          "id": "30d56ffb022f8f692f0632f",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Machine learning on Apache Apex with Apache Samoa",
          "speaker": "Bhupesh Chawda",
          "accepted": true,
          "submitter": "6047a505b705ffbde28efecdd7b7e0224ef7388eb1fc81d88477d0ff"
        },
        "room": "17989985db1fe0ab8",
        "day": 1479340800,
        "assignee": "30d56ffb022f8f692f0632f",
        "title": null,
        "endtime": 1479383400
      }, {
        "starttime": 1479384000,
        "origin": "17989985db1fe0ab8",
        "id": "dd700115b4d53c9cc",
        "duration": 50,
        "readonly": false,
        "request_id": "dd700115b4d53c9cc",
        "talk": {
          "description": "Performance Tuning tips for Apache Spark Machine Learning workloads - OpenPOWER 8 architecture is the latest offering of IBM SoftLayer, is the perfect platform for evaluating and optimizing Apache Spark solutions. In under 60 minutes from receiving a Sotlayer welcome package to your new bare-metal Power8 server, you can have Hadoop and Spark, along with many other software applications, installed, configured, optimized, and ready to run Spark ML workload. In this talk we will cover: 1) Apache Spark overview  2) Apache Spark software deployment  3) Spark optimization on highly threaded server 4) Demo\" ",
          "onhold": false,
          "bio": "Shreeharsha GN has many years of experience in the field of Performance Engineering for software applications and Java stack optimization, big data software and  IBM java stack performance optimization at companies including IBM, Azul systems , HCL, Infosys. He is a SPEC member and was a key person responsible for  certifying energy efficiency of IBM enterprise servers. At present, he is working on Apache spark Open Power enablement  and Apache spark big data performance optimization on IBM platform.",
          "request_id": "c4a1c4156d573e56debf5d1",
          "category": "ML",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.5505,
          "notes": "",
          "tags": "",
          "id": "c4a1c4156d573e56debf5d1",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Performance Tuning tips for Apache Spark Machine Learning workloads",
          "speaker": "lambzee gn",
          "accepted": true,
          "submitter": "431c2beb393daabb25f448e5568e3c213c8d330976e76035659cc64e"
        },
        "room": "17989985db1fe0ab8",
        "day": 1479340800,
        "assignee": "c4a1c4156d573e56debf5d1",
        "title": null,
        "endtime": 1479387000
      }, {
        "starttime": 1479387600,
        "origin": "17989985db1fe0ab8",
        "id": "7bd114866724e6b89",
        "duration": 50,
        "readonly": false,
        "request_id": "7bd114866724e6b89",
        "talk": {
          "description": "Big Data applications rely on machine learning to derive new value. Model training and deployment are handled by different people in different environments, which makes model transferability a major concern.\n\n\n\nThis talk inquires into popular R, Scikit-Learn and Apache Spark model types, and connects them at a standardized PMML representation level. PMML adds value to all stages of the workflow, starting from model interpretation, reorganization and persistence, and ending with fully-automated model deployment to schema-full Big Data frameworks.\n\n\n\nAttendees will learn that models are not locked-in \"black boxes\", but easily accessible and programmable components in the application layer. This realization should translate to improved workflows, and smarter and more performant applications.",
          "onhold": false,
          "bio": "Villu Ruusmann is the founder and CTO of Openscoring Ltd, a company provides an open source implementation of the Predictive Model Markup Language (PMML) standard. Villu has extensive knowledge about popular machine learning model training and deployment platforms, which he has turned into a mass of PMML-based integration solutions.\n\n\n\nVillu has limited public speaking experience from his academic pursuits. The latest talk was held at Hadoop Summit 2016 Dublin.",
          "request_id": "770e4613f40455bd0cf56f3",
          "category": "ML",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.5259,
          "notes": "",
          "tags": "",
          "id": "770e4613f40455bd0cf56f3",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "On the representation and reuse of Machine Learning models",
          "speaker": "Villu Ruusmann",
          "accepted": true,
          "submitter": "5f216ddd492f2be2ee1869661aea58710e7803386dfb75c6175d47e0"
        },
        "room": "17989985db1fe0ab8",
        "day": 1479340800,
        "assignee": "770e4613f40455bd0cf56f3",
        "title": null,
        "endtime": 1479390600
      }]
    }, {
      "day": 1479254400,
      "slots": [{
        "starttime": 1479294000,
        "origin": "17989985db1fe0ab8",
        "id": "9e0349ac26b4d7308",
        "duration": 50,
        "readonly": false,
        "request_id": "9e0349ac26b4d7308",
        "talk": {
          "description": "Even after a century of the Industrial Revolution, manufacturing processes even within assembly lines, involve manual steps requiring costly human intervention. Eg:Product quality inspection. With the advent of machine learning and big data tools, it has become possible to automate many of these manual processes. What is more, such solutions can surpass human capability for manual quality inspection. In this session we will look at a few examples of how products on assembly lines can be monitored for quality, using image processing techniques combined with machine learning. The solution to be presented, is built using a combination of machine learning and deep learning techniques running on Apache Spark Streaming.\n\nThe presentation will also explain the steps involved in creating such a solution: mapping a business need to a ML based technical solution",
          "onhold": false,
          "bio": "Prajod is a senior Architect, with 20 years of experience, in the open source group of Wipro, responsible for research and solution development in the area of Big Data and Analytics.  He has presented at multiple open source conferences (Open Source India Days, Great Indian Developer Summit, JUDCon, WSO2Con). He has also written articles on technology, in online forums and print magazines.\n\nSee his Linkedin page, for presentation and article links: https:\/\/in.linkedin.com\/in\/prajod\n\n",
          "request_id": "7c8d3591262facedefdc445",
          "category": "ML",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.7119,
          "notes": "",
          "tags": "",
          "id": "7c8d3591262facedefdc445",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Smart Manufacturing with Apache Spark Streaming and Deep Learning",
          "speaker": "Prajod S V",
          "accepted": true,
          "submitter": "5dcc0a938f10f708546c0e0869ce8e54e99c0e52b0ce8823ddc29823"
        },
        "room": "17989985db1fe0ab8",
        "day": 1479254400,
        "assignee": "7c8d3591262facedefdc445",
        "title": null,
        "endtime": 1479297000
      }, {
        "starttime": 1479297600,
        "origin": "17989985db1fe0ab8",
        "id": "4a9685e3d4d1ccc44",
        "duration": 50,
        "readonly": false,
        "request_id": "4a9685e3d4d1ccc44",
        "talk": {
          "description": "Data scientists love tools like R and Scikit-Learn since they are declarative and offer convenient and intuitive syntax for analysis tasks but are limited by local memory, Mahout offers similar features with near seamless distributed execution.\n\nIn this talk, we will look at Mahout-Samsara's distributed linear algebra capabilities and demonstrate the same by building a classification algorithm for the popular 'Eigenfaces' problem using the Samsara DSL from an Apache Zeppelin notebook. We will demonstrate how a simple classification algorithm may be prototyped and executed, and show the performance using Samsara DSL with GPU acceleration. This will demonstrate how ML algorithms built with Samsara DSL are automatically parallelized and optimized to execute on Apache Flink and Apache Spark without the developer having to deal with the underlying semantics of the execution engine.",
          "onhold": false,
          "bio": "Suneel Marthi is a Principal Engineer in the Office of Technology, Redhat Inc. and a member of the Apache Software Foundation, and a Committer and PMC member on Apache Mahout and Apache Pirk.  He has previously presented at Apache Big Data, Hadoop Summit Europe and Flink Forward. ",
          "request_id": "f87b6fba178fbf6c8e7b145",
          "category": "ML",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.5987,
          "notes": "",
          "tags": "",
          "id": "f87b6fba178fbf6c8e7b145",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Native and Distributed Machine Learning with Apache Mahout",
          "speaker": "Suneel Marthi",
          "accepted": true,
          "submitter": "f02b1143e82dc02f398b3255cb3855efd36be4500b571f331db7ae1d"
        },
        "room": "17989985db1fe0ab8",
        "day": 1479254400,
        "assignee": "f87b6fba178fbf6c8e7b145",
        "title": null,
        "endtime": 1479300600
      }, {
        "starttime": 1479301200,
        "origin": "17989985db1fe0ab8",
        "id": "d8e973abdfe674c6c",
        "duration": 50,
        "readonly": false,
        "request_id": "d8e973abdfe674c6c",
        "talk": {
          "description": "Text is one of the most used forms of communication and ubiquitous in the Internet. Social networks like Facebook and Twitter mainly contain unstructured text; the same is true for content-driven websites.\n\n\n\nFor humans it is easy to grasp the meaning of text - much more difficult for computers. Used correctly, computers can help humans tremendously in structuring and classifying huge amounts of text. This \"symbiosis\" can help humans work more efficiently, reduce repetitve work and use the uncovered structure.\n\n\n\nOur talk starts with visualizations giving us ideas how to automatically classify texts. Then we will demonstrate that  manual intervention is sometimes necessary and how this can be used as a basis for machine learning. This helps significantly in classifying more complicated cases.\n\n\n\nAs software tools we use R, Apache Solr, D3.js, and several NLP and ML tools from the ASF.",
          "onhold": false,
          "bio": "Christian has worked for 20 years with Internet technologies. Recently, he has focused on working with large amounts of data or many users. As big data applications become more and more popular, lots of applications evolve. Many aggregates have to be calculated to describe charcteristics of data sets. This is why he concentrates on intelligent algorithms like machine learning to find those interpretations. Often he uses sophisticated visualization to detect the underlying structure.",
          "request_id": "6641e90b20b1e33018492b1",
          "category": "ML",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.6705,
          "notes": "",
          "tags": "",
          "id": "6641e90b20b1e33018492b1",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Classifying unstructured text - deterministic and machine learning approaches",
          "speaker": "Christian Winkler",
          "accepted": true,
          "submitter": "7efefcc9674518ca6aa175f625a0a7afcf7293abcc8a23aa235f8aaa"
        },
        "room": "17989985db1fe0ab8",
        "day": 1479254400,
        "assignee": "6641e90b20b1e33018492b1",
        "title": null,
        "endtime": 1479304200
      }, {
        "starttime": 1479304200,
        "origin": "17989985db1fe0ab8",
        "id": "ff1bcde1ffbd09e28",
        "duration": 100,
        "readonly": true,
        "request_id": "ff1bcde1ffbd09e28",
        "room": "17989985db1fe0ab8",
        "day": 1479254400,
        "title": "Lunch",
        "endtime": 1479310200
      }, {
        "starttime": 1479310200,
        "origin": "17989985db1fe0ab8",
        "id": "aa8d71f697c67aa58",
        "duration": 50,
        "readonly": false,
        "request_id": "aa8d71f697c67aa58",
        "talk": {
          "description": "We would like to present our open source AMIDST toolbox for analysis of large-scale data sets using probabilistic machine learning models. AMIDST runs algorithms in a distributed fashion for learning a wide range of latent variable models such as Gaussian mixtures, (probabilistic) principal component analysis, Hidden Markov Models, Kalman Filter, Latent Dirichlet Allocation, etc. This toolbox is able to learn any user-defined probabilistic (graphical) model with billions of nodes using novel message passing algorithms.\n\n\n\nWe plan to give an overview of the AMIDST toolbox, some details about the API and the integration with Flink, Spark (and other open source tools) and an analysis of the scalability of our learning algorithms. All this in the context of a real use case scenario in the financial domain (BCC group), where millions of customers profiles are analyzed.",
          "onhold": false,
          "bio": "I am a research fellow at NTNU (Norway) with broad interests in data mining and machine learning using probabilistic graphical models. Lately, my research has focused on scalable machine learning methods for solving real use cases in the financial (BCC group) and automotive industry (Daimler group). I am coauthor of more than 50 scientist papers in different journal and international conferences covering applied areas such as bioinformatics, information retrieval, crime prediction and sales forces allocation.\n\n\n\nI have been speaker in dozens of international conferences related to machine learning research over the last fifteen years. Recently, our team has been invited to present the AMIDST toolbox at the open source conference Flink-Forward 2016. ",
          "request_id": "28cce58fb7f342410cbccab",
          "category": "ML",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.7276,
          "notes": "",
          "tags": "",
          "id": "28cce58fb7f342410cbccab",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "AMIDST Toolbox: A Java Toolbox for Scalable Probabilistic Machine Learning",
          "speaker": "Andres  Masegosa",
          "accepted": true,
          "submitter": "a23a7b538cb5dc8716c463622d646411b3c3f24b85d02cfbb9b8978f"
        },
        "room": "17989985db1fe0ab8",
        "day": 1479254400,
        "assignee": "28cce58fb7f342410cbccab",
        "title": null,
        "endtime": 1479313200
      }, {
        "starttime": 1479313800,
        "origin": "17989985db1fe0ab8",
        "id": "279e62020978c0994",
        "duration": 50,
        "readonly": false,
        "request_id": "279e62020978c0994",
        "talk": {
          "description": "Data scientists care about statistics and fast iteration cycles for their experiments. They should not be concerned with technicalities like hardware failures, tenant isolation, or low cluster utilization. In order to shield its data scientists from these matters, Blue Yonder is using Apache Aurora.\n\n\n\nWhen adopting Aurora, our goal was to run multiple machine learning projects on the same physical cluster. This talk will go into details of this adoption process and highlight key engineering decisions we have made. Particular focus will reside on the multi-tenancy and oversubscription features of Apache Aurora and Apache Mesos, its underlying resource manager.\n\n\n\nAudience members will learn about the fundamentals of both Apache projects and how those can be assembled into a capable machine learning platform.",
          "onhold": false,
          "bio": "Stephan Erb is a software engineer driven by the goal to make Blue Yonder's data scientists more productive. Stephan holds a master's degree in computer science from the Karlsruhe Institute of Technology (KIT). He is a PMC member of the Apache Aurora project and tweets at @ErbStephan.\n\n",
          "request_id": "3cdf243357833275c439a5e",
          "category": "ML",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.8392,
          "notes": "",
          "tags": "",
          "id": "3cdf243357833275c439a5e",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Multi-tenant Machine Learning with Apache Aurora and Apache Mesos",
          "speaker": "Stephan Erb",
          "accepted": true,
          "submitter": "7d480290319aa8c20243d050a8428f5eef625cf6d4ff477f16824837"
        },
        "room": "17989985db1fe0ab8",
        "day": 1479254400,
        "assignee": "3cdf243357833275c439a5e",
        "title": null,
        "endtime": 1479316800
      }]
    }, {
      "day": 1479168000,
      "slots": [{
        "starttime": 1479207600,
        "origin": "17989985db1fe0ab8",
        "id": "ecbcc1cc593c617f6",
        "duration": 50,
        "readonly": false,
        "request_id": "ecbcc1cc593c617f6",
        "talk": {
          "description": "Data science is moving with gusto to the enterprise, where data often resides in relational databases with SQL as the main workload.  So how can an enterprise add a data science dimension to their business without a major IT re-architecture?\n\n\n\nApache MADlib (incubating) is an innovative SQL-based open source library for scalable in-database analytics.  It provides parallel implementations of mathematical, statistical and machine learning methods.   Bringing machine learning computations to the data makes for excellent scale out performance on massively parallel processing (MPP) platforms like Greenplum database and Apache HAWQ (incubating).\n\n\n\nIn this talk, we will describe the origin of MADlib, review the architecture and common usage patterns, and look ahead to some interesting plans around performance acceleration. \n\n",
          "onhold": false,
          "bio": "Frank McQuillan is Director of Product Management at Pivotal, focusing on analytics and machine learning for large data sets.  He is a committer on the Apache MADlib (incubating) project and speaks regularly at conferences and meet-ups, including recently at FOSDEMäó»16.\n\n\n\nPrior to Pivotal, Frank worked on projects in the areas of robotics, drones, flight simulation, and advertising technology.  He holds a Masters degree from the University of Toronto and a Bachelor's degree from the University of Waterloo, both in Mechanical Engineering.\n\n",
          "request_id": "d021826a1f2c1f77e434a0f",
          "category": "ML",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.4944,
          "notes": "",
          "tags": "",
          "id": "d021826a1f2c1f77e434a0f",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Distributed In-Database Machine Learning with Apache MADlib (incubating)",
          "speaker": "Frank McQuillan",
          "accepted": true,
          "submitter": "5714f03a359c29fa567fa166d9b2197f8024c50ca0ccfdd5669109b1"
        },
        "room": "17989985db1fe0ab8",
        "day": 1479168000,
        "assignee": "d021826a1f2c1f77e434a0f",
        "title": null,
        "endtime": 1479210600
      }, {
        "starttime": 1479211200,
        "origin": "17989985db1fe0ab8",
        "id": "1915f9570a8ea571b",
        "duration": 50,
        "readonly": false,
        "request_id": "1915f9570a8ea571b",
        "talk": {
          "description": "There are many Machine Learning projects available: Apache Mahout, Apache SystemML (incubating), Apache Spark MLlib, Tensorflow, Scikit-learn, etc. \n\nIn this session we are going to showcase how typical ML predictive analytics workflow can benefit from modern notebooks-style interactive environment like Apache Zeppelin. We going to share examples of successful integration between different project in big-data ecosystem and touch up on various techniques like visual recognition, NLP, and Deep Learning.\n\n",
          "onhold": false,
          "bio": "Alexander Bezzubov is Apache Zeppelin contributor, PMC member and software engineer at NFLabs.\n\nPrevious speaking experience includes Apache BigData NA 2016 in Vancouver, FOSSASIA 2016 in Singapore, Apache BigData EU 2015 in Budapest.",
          "request_id": "14c9a16aa925b755222404a",
          "category": "ML",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.6651,
          "notes": "",
          "tags": "",
          "id": "14c9a16aa925b755222404a",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Machine Learning in Apache Zeppelin",
          "speaker": "Alexander Bezzubov",
          "accepted": true,
          "submitter": "55080f947af914e816fad5d1ee275e946873e162291059bb8d04f84e"
        },
        "room": "17989985db1fe0ab8",
        "day": 1479168000,
        "assignee": "14c9a16aa925b755222404a",
        "title": null,
        "endtime": 1479214200
      }, {
        "starttime": 1479214800,
        "origin": "17989985db1fe0ab8",
        "id": "d33154c1e97c4224e",
        "duration": 50,
        "readonly": false,
        "request_id": "d33154c1e97c4224e",
        "talk": {
          "description": "Apache PredictionIO (incubating) provides a full stack machine learning environment on top of Apache Spark, making it easy for developers to iterate on production-deployable machine learning engines.  Apache PredictionIO is designed for data scientists and developers to build predictive web services for real-world applications in a fraction of the time normally required. \n\n\n\nIn this talk, the speaker will introduce the latest developments of PredictionIO, and show how to use it to build and deploy predictive engines in real production environments. Using PredictionIOäó»s DASE design pattern, Simon will illustrate how developers can build machine learning applications with the separation of concerns (SoC) in mind. The speaker will also go over the future roadmap of Apache PredictionIO and some of its recent development. \n\n",
          "onhold": false,
          "bio": "Simon Chan is an open-source product innovator, with 13 years of tech management experience in various countries. He is the founder and former CEO of the company that created PredictionIO - currently ranked on Github as the most popular Spark-based machine learning OSS project in the world. PredictionIO company is acquired by Salesforce in 2016 and the open-source product has been accepted by ASF as Apache PredictionIO (incubating).  Simon is currently the Senior Director in Product Management and leads data science product development. \n\n\n\nSimon has a B.S.E. in Computer Science from University of Michigan, Ann Arbor and a PhD in Machine Learning from University College London.",
          "request_id": "afabf54c0b787ab28114e00",
          "category": "ML",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.9744,
          "notes": "",
          "tags": "",
          "id": "afabf54c0b787ab28114e00",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Apache PredictionIO: End-to-End Machine Learning Server",
          "speaker": "Simon Chan",
          "accepted": true,
          "submitter": "a75cd221f602201e25956dd41bb82d8d5711b2e181a63289e5f4e6d3"
        },
        "room": "17989985db1fe0ab8",
        "day": 1479168000,
        "assignee": "afabf54c0b787ab28114e00",
        "title": null,
        "endtime": 1479217800
      }, {
        "starttime": 1479217800,
        "origin": "17989985db1fe0ab8",
        "id": "1cd94bd8a71e8df31",
        "duration": 100,
        "readonly": true,
        "request_id": "1cd94bd8a71e8df31",
        "room": "17989985db1fe0ab8",
        "day": 1479168000,
        "title": "Lunch",
        "endtime": 1479223800
      }, {
        "starttime": 1479223800,
        "origin": "17989985db1fe0ab8",
        "id": "2adf030d6e7156b16",
        "duration": 50,
        "readonly": false,
        "request_id": "2adf030d6e7156b16",
        "talk": {
          "description": "How to use text data to draw conclusions about users of our website or forum?\n\nThis talk describes solution of particular problem with use of Machine Learning and Statistics. Based on provided forum we will create the program that learn structure of posts using Natural Language Processing technics. Then after proper Machine Learning models were trained, program is able to answer for given text with given probability which of authors from the forum wrote this post.\n\nWe will go through all steps required to create Machine Learning models for text.\n\nHow to use Natural Language Processing and Bag-of-Words techniques to analyse text?\n\nHow to prepare input data to further Processing by Machine Learning Models?\n\nThose questions will be answered. Implementation will be written in Apache Spark, so we will get to know that technology with some important libraries like Spark MLlib and DataFrame API",
          "onhold": false,
          "bio": "Software Engineer working at Allegro Group, programming mostly in Java, Scala. Fan of microservices architecture, and functional programming. I am dedicated considerable time and effort to be better every day. Recently diving into Big Data technologies such as Apache Spark and Hadoop. I am passionate about nearly everything associated with software development. Thinking that we should always try to consider different solutions, and approach before solving a problem. Recently I was a speaker at conferences in Poland - Confitura and JDD (Java Developers Day) and also at Krakow Scala User Group. Iäó»ve also conducted live coding session at Geecon Conference.\n\n\n\nJDD: https:\/\/www.youtube.com\/watch?v=xydpSPdglQw\n\nGeecon: https:\/\/vimeo.com\/132288070\n\nConfitura (lecture in polish language) https:\/\/www.youtube.com\/watch?v=_Cu8hc2Z5iQ",
          "request_id": "3621311220458afaf4ab61d",
          "category": "ML",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.3134,
          "notes": "",
          "tags": "",
          "id": "3621311220458afaf4ab61d",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Machine Learning with Apache Spark",
          "speaker": "Tomasz Lelek",
          "accepted": true,
          "submitter": "d42cdc501f7c3cb913850fb4ee2d7c332ef941c5f9971d061a766881"
        },
        "room": "17989985db1fe0ab8",
        "day": 1479168000,
        "assignee": "3621311220458afaf4ab61d",
        "title": null,
        "endtime": 1479226800
      }, {
        "starttime": 1479227400,
        "origin": "17989985db1fe0ab8",
        "id": "973f9ca91f7cb9109",
        "duration": 50,
        "readonly": false,
        "request_id": "973f9ca91f7cb9109",
        "talk": {
          "description": "Machine learning in the enterprise is an iterative process. Data scientists will tweak or replace their learning algorithm in a small data sample until they find an approach that works for the business problem and then apply the Analytics to the full data set. Apache SystemML is a new system that accelerates this kind of exploratory algorithm development for large-scale machine learning problems.Think of SystemML as SQL for Machine Learning, it provides a high-level language to quickly implement and run algorithms, and it also enable cost-based optimizer that takes care of low-level decisions about parallelism, allowing users to focus on the algorithm and the real-world problem that the algorithm is trying to solve. This talk will introduce you to SystemML This talk will introduce you to SystemML and get you started building declarative analytics with SystemML using a Zeppelin notebooks.",
          "onhold": false,
          "bio": "Luciano Resende is an Architect in IBM Analytics. He has been contributing to open source at The ASF for over 10 years, he is a member of ASF and is currently contributing to various big data related Apache projects including Spark, Zeppelin, Bahir. Luciano is the project chair for Apache Bahir, and also spend time as mentor newly created  Apache Incubator projects. At IBM, he contributed to several IBM big data offerings, including BigInsights, IOP and its respective Bluemix Cloud services.",
          "request_id": "c5971fac2f21c9bed50afca",
          "category": "ML",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.8442,
          "notes": "",
          "tags": "",
          "id": "c5971fac2f21c9bed50afca",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "SystemML - Declarative Machine Learning",
          "speaker": "Luciano Resende",
          "accepted": true,
          "submitter": "e75be01b191373cd5af79fbbe3c5bec34983c210ba6e9a27e8e8a207"
        },
        "room": "17989985db1fe0ab8",
        "day": 1479168000,
        "assignee": "c5971fac2f21c9bed50afca",
        "title": null,
        "endtime": 1479230400
      }]
    }]
  }, {
    "id": "d2833fd70409436c6",
    "name": "Giralda V",
    "days": [{
      "day": 1479340800,
      "slots": [{
        "starttime": 1479380400,
        "origin": "d2833fd70409436c6",
        "id": "5c3243108a0af7a78",
        "duration": 50,
        "readonly": false,
        "request_id": "5c3243108a0af7a78",
        "talk": {
          "description": "Implementing use cases on unified data platforms. Having a unified data processing engine empowers Big Data application developers as it makes connections between seemingly unrelated use cases natural. This talk discusses the implementation of the so-called BigPetStore project (which is a part of Apache Bigtop) in Apache Spark and Apache Flink. The aim BigPetStore is to provide a common suite to test and benchmark Big Data installations. The talk features best practices and implementation with the batch, streaming, SQL, DataFrames and machine learning APIs of Apache Spark and Apache Flink side by side. A range of use cases are outlined in both systems from data generation, through ETL, recommender systems to online prediction.",
          "onhold": false,
          "bio": "Míçrton Balassi is a Solution Architect at Cloudera and a PMC member at Apache Flink. He focuses on Big Data application development, especially in the streaming space. Marton is a regular contributor to open source and has been a speaker of a number of Big Data related conferences and meetups, including Hadoop Summit and Apache Big Data recently.\n\nMíçrton has been a speaker at ApacheCon, Hadoop Summit and numerous Big Data related meetups recently.",
          "request_id": "74e0c35dde64ee7def838b0",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.3272,
          "notes": "",
          "tags": "",
          "id": "74e0c35dde64ee7def838b0",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Implementing BigPetStore in Spark and Flink",
          "speaker": "Míçrton Balassi",
          "accepted": true,
          "submitter": "312fcf9e9932e7e2738088f3d08198d8221052c646ee518a09119e1f"
        },
        "room": "d2833fd70409436c6",
        "day": 1479340800,
        "assignee": "74e0c35dde64ee7def838b0",
        "title": "11:00 -> 11:50 (50 min)",
        "endtime": 1479383400
      }, {
        "starttime": 1479384000,
        "origin": "d2833fd70409436c6",
        "id": "d7e8232da7884538f",
        "duration": 50,
        "readonly": false,
        "request_id": "d7e8232da7884538f",
        "talk": {
          "description": "All kinds of data volume increases dramatically in recent years, new storage devices (NVMe SSD, flash SSD, etc.) can be utilized to improve data access performance. HDFS provides methodologies like HDFS Cache, Heterogeneous Storage Management (HSM) and Erasure Coding (EC) to provide such support, but it remains a big challenge to define and adjust different storage strategies for different data in a dynamic environment. \n\nTo overcome the challenge and improve the storage efficiency of HDFS, we will introduce a comprehensive solution, aka Smart Storage Management (SSM) in Apache Hadoop. HDFS operation data and system state information are collected from the cluster, based on the metrics collected SSM can extract some äóìdata access patternsäó and based on these patterns SSM will automatically make sophisticated usage of these methodologies to optimize HDFS storage efficiency.",
          "onhold": false,
          "bio": "Software engineer in Intel. Currently mainly focus on Apache Hadoop performance optimization. Co-speaker on HBase Developer Course in Strata+Hadoop world Beijing 2016.\n\n",
          "request_id": "3344a599170f3654b3006bf",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.74,
          "notes": "",
          "tags": "",
          "id": "3344a599170f3654b3006bf",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Smart Storage Management: Towards Higher HDFS Storage Efficiency",
          "speaker": "Wei Zhou",
          "accepted": true,
          "submitter": "a92cc0018d44a6408210a7de24ab946e047c7ca8111422c51e46f6cb"
        },
        "room": "d2833fd70409436c6",
        "day": 1479340800,
        "assignee": "3344a599170f3654b3006bf",
        "title": "12:00 -> 12:50 (50 min)",
        "endtime": 1479387000
      }, {
        "starttime": 1479387600,
        "origin": "d2833fd70409436c6",
        "id": "78a65e986c1f917c0",
        "duration": 50,
        "readonly": false,
        "request_id": "78a65e986c1f917c0",
        "talk": {
          "description": "Hadoop ecosystem are getting more and more expensive in terms of deployment and operations. To leverage the big data infrastructure, we have to handle the issues like multi-tenant, upgrade and scalability by ourselves. Luckily, Docker as a cloud engine facilitate us with ability to bring Hadoop fly onto the cloud. By using Docker, weäó»re able to manage the CPU resource, network resource and disk resource easily. In our talk, we will demonstrate how we build a Hadoop cluster beyond a Docker in our practice. We will give you an in-depth tour how to build a cloud based Hadoop cluster with well managed resources like CPUs, network and disks. We will show you the PROs and CONs building the system on different network modes provided by Docker including host, overlay and port mapping. Besides that, we will show you some experiences to build Hadoop on Docker in a Calico managed environment.",
          "onhold": false,
          "bio": "I am a software engineer from Intel. I am now working on Apache Hive project, Apache Parquet and Apache Spark Project. I am a committer of Apache HIVE project. Now I am focussed on Spark Authorization specially in Spark SQL component and the performance improvements in Apache Parquet project.",
          "request_id": "7e54eb523fdade02869379f",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.5522,
          "notes": "",
          "tags": "",
          "id": "7e54eb523fdade02869379f",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Bring Hadoop to the Cloud: A practice of Hadoop on Docker",
          "speaker": "cheng xu",
          "accepted": true,
          "submitter": "4e53b37ed48427f53d8085e77f8be8d41046c7d60c2c77f3e96eeaf2"
        },
        "room": "d2833fd70409436c6",
        "day": 1479340800,
        "assignee": "7e54eb523fdade02869379f",
        "title": "13:00 -> 13:50 (50 min)",
        "endtime": 1479390600
      }]
    }, {
      "day": 1479254400,
      "slots": [{
        "starttime": 1479294000,
        "origin": "d2833fd70409436c6",
        "id": "18cf04290152568aa",
        "duration": 50,
        "readonly": false,
        "request_id": "18cf04290152568aa",
        "talk": {
          "description": "As modern enterprises migrate to microservice-centric cloud architecture, it has become imperative to build a new data analysis framework to handle äóñ often in real time - the event-based data these services produce. For this presentation, we will demonstrate how to leverage multiple open source projects to build a robust framework quickly and cheaply that can scale with an organization as it grows and inexorably generates more and more data.\n\nThey will cover a tangible, real-world, implementation that includes Apache technologies such as Kafka, Mesos, and Spark, as well as open-source PrestoDB (Facebook).\n\n \n\nThe speakers will discuss lessons learnt during and after the build, as well as\n\nsome specific use-cases for how this approach brought about otherwise-unattainable actionable business insights and results, including hardware failure prediction and capacity planning.",
          "onhold": false,
          "bio": "Dao Mi has extensive experience working with data of different scale, type and velocity across myriad industries, from natural gas to floating bonds. While at Microsoft, he helped deliver BI and predictive analytics solutions to Fortune 500 clients.  He has helped build a large custom analytics platform for use across DigitalOcean, a fast-growing global cloud hosting company, where he works presently. He has spoken previously at TechReady for Microsoft on R integration with Excel. \n\n\n\nAlex Kass has worked at companies ranging from large financial institutions to early-stage startups, regularly building successful analytical models and systems of varying size. Now at DigitalOcean, he has at his disposal sufficient software and hardware firepower (& startup autonomy) to experiment and build with both stable and cutting edge technologies, delivering actionable statistical insights at scale.",
          "request_id": "3ca6f2219d25d91519de256",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.9794,
          "notes": "",
          "tags": "",
          "id": "3ca6f2219d25d91519de256",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "\"Building a Robust Analytics Platform with an Open-Source Stack",
          "speaker": "Dao Mi",
          "accepted": true,
          "submitter": "e5b4c9a7badde26bf6f5a93cb0bb4e73812ab5a51b712d4521f8db30"
        },
        "room": "d2833fd70409436c6",
        "day": 1479254400,
        "assignee": "3ca6f2219d25d91519de256",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479297000
      }, {
        "starttime": 1479297600,
        "origin": "d2833fd70409436c6",
        "id": "f959d6e168858a4f6",
        "duration": 50,
        "readonly": false,
        "request_id": "f959d6e168858a4f6",
        "talk": {
          "description": "Apache Ignite is one of the fastest growing apache projects. The presentation will take the audience on a roadmap discovery of Ignite moving to a converged storage model, supporting both, analytical and transactional data sets. We will go over the differences between Fast Data and Big Data and cover the projects supporting both technologies. We will discuss the reasons, real-life use cases and technology approaches for merging Fast Data and Big Data in order to deliver a consistent & universal data processing platform regardless of where data resides relative to HDD, flash or DRAM.",
          "onhold": false,
          "bio": "Dmitriy Setrakyan is founder and Chief Product Officer at GridGain. Dmitriy has been working with distributed architectures for over 15 years and has expertise in the development of various middleware platforms, financial trading systems, CRM applications and similar systems. Prior to GridGain, Dmitriy worked at eBay where he was responsible for the architecture of an add-serving system processing several billion hits a day. Currently Dmitriy also acts as PMC chair of Apache Ignite project.",
          "request_id": "a2a336ac2ecb6238d5c7cc3",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.3038,
          "notes": "",
          "tags": "",
          "id": "a2a336ac2ecb6238d5c7cc3",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Apache Ignite - Path to Converged Data Platform",
          "speaker": "Dmitriy Setrakyan",
          "accepted": true,
          "submitter": "e0965fa7a43700cd60d504e443df988bde5d42810eb64074228fd394"
        },
        "room": "d2833fd70409436c6",
        "day": 1479254400,
        "assignee": "a2a336ac2ecb6238d5c7cc3",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479300600
      }, {
        "starttime": 1479301200,
        "origin": "d2833fd70409436c6",
        "id": "8e3ade2d4106522c2",
        "duration": 50,
        "readonly": false,
        "request_id": "8e3ade2d4106522c2",
        "talk": {
          "description": "Apache Pig is a popular scripting platform for processing and analyzing large data sets in the Hadoop ecosystem. With its open architecture and backend neutrality, Pig scripts can currently run on MapReduce and Tez. Apache Spark is an open-source data analytics cluster computing framework that has gained significant momentum recently. Besides offering performance advantages, Spark is also a more natural fit for the query plan produced by Pig. Pig on Spark enables improved ETL performance while also supporting users intending to standardize to Spark as the execution engine.",
          "onhold": false,
          "bio": "Liyun Zhang is a Software Engineer at Intel. She is one of main contributors of Pig on Spark project. Prior to that, she made several contributions to Intel Distribution for Hadoop.",
          "request_id": "7df9aaecc320c808e6cc62e",
          "category": "spark",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.1983,
          "notes": "",
          "tags": "",
          "id": "7df9aaecc320c808e6cc62e",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Power Pig with Spark",
          "speaker": "Liyun zhang",
          "accepted": true,
          "submitter": "9f6a61dde5b0f838c23bf13570e5a8df171dc82c513b969df085b943"
        },
        "room": "d2833fd70409436c6",
        "day": 1479254400,
        "assignee": "7df9aaecc320c808e6cc62e",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479304200
      }, {
        "starttime": 1479310200,
        "origin": "d2833fd70409436c6",
        "id": "11293bd5ba6966ae5",
        "duration": 50,
        "readonly": false,
        "request_id": "11293bd5ba6966ae5",
        "talk": {
          "description": "There is always a tussle when it comes which is better when we talk about Mesos and YARN. It always happens that the users of Mesos are not much aware of YARN and vice versa and comparison just happens at the superficial level.\n\nAs part of this presentation, we want to provide overview of architectures of both ResourceManagement Frameworks, And then compare each of them functionally.\n\nWe also plan to present which suits better in which scenarios and breif overview of collaborations projects which are existing currently.",
          "onhold": false,
          "bio": "I am BigData Enthusiast and have experience in developing BigData Hadoop applications and platforms since 4 years. I have 12 years of experience as a Java Software Developer.\n\nI have been actively contributing for Hadoop YARN and Map Reduce since 2 years and currently Apache Hadoop  Commiter. I have actively particpated in Hadoop YARN Node Labels and ATS features. \n\nFurther details : http:\/\/people.apache.org\/~naganarasimha_gr\/ & http:\/\/in.linkedin.com\/in\/naganarasimha-garla-a620297",
          "request_id": "c01bd35174022e14a6b87f8",
          "category": "intro",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.4603,
          "notes": "",
          "tags": "",
          "id": "c01bd35174022e14a6b87f8",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Mesos vs YARN, who fits best and when?",
          "speaker": "Naganarasimha Garla",
          "accepted": true,
          "submitter": "a9f1cf4c9f3f347af2a0dc0e25288e3045c1188b499078f4cb1c1c15"
        },
        "room": "d2833fd70409436c6",
        "day": 1479254400,
        "assignee": "c01bd35174022e14a6b87f8",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479313200
      }, {
        "starttime": 1479313800,
        "origin": "d2833fd70409436c6",
        "id": "dc44b47783865213f",
        "duration": 50,
        "readonly": false,
        "request_id": "dc44b47783865213f",
        "talk": {
          "description": "Julien will present Marmot, a system built at Swisscom to do real-time anomaly detection on time series. Marmot uses a combination of machine learning and big data technologies in order to trigger alerts in case of problems in Swisscom network.\n\nMarmot monitors arbitrary time series and trains statistical models that can be used to spot anomalies from both batch (historical) and streaming (live) data. It is composed of a Python modules for anomaly detection and data ingestion from Druid, as well as Scala modules using Apache Spark for ingesting from Apache Kafka and Apache Hadoop's HDFS.\n\nMarmot is currently successfully used at Swisscom to trigger alerts in case of problems with VoIP calls, which represent more than 3 millions phone calls per day.\n\nThis is joint work with Khue Vu, who worked on Marmot for his MSc thesis at EPFL, and the network intelligence team of Swisscom Innovation.",
          "onhold": false,
          "bio": "Julien is a data scientist at Swisscom. His experience lies in the areas of machine learning and network algorithms, and his current work includes building analytics and monitoring platforms using big data technologies such as Apache Spark, Druid and Apache Cassandra. He has a PhD from EPFL and talked at various venues such as IEEE Infocom, IEEE ICNP, ACM Mobicom and the Swiss machine learning days.",
          "request_id": "d65966d06cb6390ee184382",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.1832,
          "notes": "",
          "tags": "",
          "id": "d65966d06cb6390ee184382",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Marmot: Anomaly Detection as a Service",
          "speaker": "Julien Herzen",
          "accepted": true,
          "submitter": "4bfc708e9edfa66fd4043c97c2f8754afa26c2520450754c865b5679"
        },
        "room": "d2833fd70409436c6",
        "day": 1479254400,
        "assignee": "d65966d06cb6390ee184382",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479316800
      }]
    }, {
      "day": 1479168000,
      "slots": [{
        "starttime": 1479207600,
        "origin": "d2833fd70409436c6",
        "id": "5e5dd9c43571b7979",
        "duration": 50,
        "readonly": false,
        "request_id": "5e5dd9c43571b7979",
        "talk": {
          "description": "Showcase how to use OCR - Optical Character Recognition technology along with Apache SOLR Search and Apache Spark to utilize text mining capabilities. A very common scenario is to be able to index and search text in image files that were scanned in, for example patient charts, legal documents, etc. In this session we will demonstrate how to use OCR technology to convert scanned documents (jpg, gif, tiff,etc.) to text documents. The converted result text data than can be stored in a HIVE, HBase, SOLR and than can be used further for Data Analysis and Exploration. We will demonstrate how to Apache Spark to text mine the data.",
          "onhold": false,
          "bio": "Alex Zeltov is Solutions Engineer \/ Software Engineer \/ Programmer Analyst \/ Data Scientist with over 15 years of industry experience in Information Technology and most recently in Big Data and Predictive Analytics. Specializing in designing, developing and implementing complex software solutions. Experienced in all areas of the software development life cycle. Currently working as Solutions Engineer at Hortonworks, where he is responsible for creating high-value Hadoop solutions for customers. He created and delivered in-depth technical demonstrations and delivered custom big data workshops, POCäó»s for new and existing accounts. He holds an M.S. in Software Engineering degree from Penn State, as well as an undergraduate degree in Computer Science from Temple University. ",
          "request_id": "27b2805d5141a15ff3af176",
          "category": "Solr",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.1321,
          "notes": "",
          "tags": "",
          "id": "27b2805d5141a15ff3af176",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Use of Apache SOLR , Apache Spark and OCR for Text Mining and Search capability for business process improvement and Advanced Analytics",
          "speaker": "alex zeltov",
          "accepted": true,
          "submitter": "b218059ac7c612eccb5ac6b0908d0c97adc2e51f07d52d766086c49c"
        },
        "room": "d2833fd70409436c6",
        "day": 1479168000,
        "assignee": "27b2805d5141a15ff3af176",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479210600
      }, {
        "starttime": 1479211200,
        "origin": "d2833fd70409436c6",
        "id": "f83568d7d17baa720",
        "duration": 50,
        "readonly": false,
        "request_id": "f83568d7d17baa720",
        "talk": {
          "description": "Apache Solr in the recent past started supporting deeply-nested documents. Solr can now be used to perform search and faceting on documents such as nested email threads, comments and replies on social media, enriched and annotated documents etc. without having to flatten them before ingestion.\n\nAnshum Gupta would discuss pre-processing of data so that it can be indexed in Solr, making it possible to perform complex search and statistical aggregation on top of it. He would also cover query formation for sample use cases of nested data and multiple options and features that Solr provides for faceting or aggregation of such documents. \n\nBy the end of this talk, Solr users would have a better understanding of both, how to work with features that Solr provides to find answers to interesting questions from deeply nested documents as well as work-arounds for the missing pieces.",
          "onhold": false,
          "bio": "Anshum Gupta is a Lucene\/Solr committer and PMC member with over 10 years of experience with search and related technologies. He is a part of the search team at IBM Watson, where he works on extending the limits and improving SolrCloud. Prior to this, he was a part of the open source team at Lucidworks and also the co-creator of AWS CloudSearch - the first search as a service offering by AWS based of Apache Solr.\n\nHe has spoken at multiple international conferences, including Apache Big Data NA, Lucene Solr Revolution, and several meetups.",
          "request_id": "71ebe680fe54bdeecc95a28",
          "category": "Solr",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.0732,
          "notes": "",
          "tags": "",
          "id": "71ebe680fe54bdeecc95a28",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Managing deeply nested documents in Apache Solr",
          "speaker": "Anshum Gupta",
          "accepted": true,
          "submitter": "ce392135cb8ad9557db7c6f52106a011f9dd27815843916acf14c322"
        },
        "room": "d2833fd70409436c6",
        "day": 1479168000,
        "assignee": "71ebe680fe54bdeecc95a28",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479214200
      }, {
        "starttime": 1479214800,
        "origin": "d2833fd70409436c6",
        "id": "d943b076776ae6c2f",
        "duration": 50,
        "readonly": false,
        "request_id": "d943b076776ae6c2f",
        "talk": {
          "description": "Email interaction has its unique characteristics and is different than traditional web search (for example in that users search their own private mailboxes and are often interested in recent emails rather than the archive). \n\nTaking advantage of these characteristics, we were able to optimize our infrastructure in terms of indexing strategy and query optimization and achieve a significant gain in scalability and performance.\n\nArnon will present the various tradeoffs that were explored, including multi-tiered indexes, sorted indexes, query optimizations and more.\n\nArnon will then present the benchmark results that stress the importance of correctly designing a Solr infrastructure and tailoring it to oneäó»s specific use case.",
          "onhold": false,
          "bio": "Arnon is a software engineer in IBM Research, part of the Social Analytics & Technologies team, Big Data and Cognitive Analytics department.\n\nArnon earned his MBA degree and his B.Sc in Computer Science from the Technion. \n\nBeing part of the Social Analytics & Technologies team, Arnonäó»s work experience includes social networks, search-based systems over Lucene and Solr and graph technologies.\n\nPrevious lecturing experience includes Connected 2015, ECSCW 2015, Connect 2016 and more.",
          "request_id": "75e73e3b792bc368a16ff43",
          "category": "Solr",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.4562,
          "notes": "",
          "tags": "",
          "id": "75e73e3b792bc368a16ff43",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Fast & scalable email system with Apache Solr - Strategies, tradeoffs and optimizations",
          "speaker": "Arnon Yogev",
          "accepted": true,
          "submitter": "cc3a5ee510e6d57b8d88aeeb5bf08867742e0189736fc05c325e6303"
        },
        "room": "d2833fd70409436c6",
        "day": 1479168000,
        "assignee": "75e73e3b792bc368a16ff43",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479217800
      }, {
        "starttime": 1479223800,
        "origin": "d2833fd70409436c6",
        "id": "53bad5ff832d3f6da",
        "duration": 50,
        "readonly": false,
        "request_id": "53bad5ff832d3f6da",
        "talk": {
          "description": "Apache Solr is widely used by organizations to power their search platforms and often support multiple users. A lot of cluster management APIs were introduced over the last few releases, allowing the users to to manage operations ranging from replica placement to forcing leader elections via API calls. At the end of this talk, intermediate Solr users would understand what's available, and when can they avoid direct interference with the system, leading to more stable clusters and lower chances of nodes going down. The attendees would also be much better equipped to build their own SolrCloud cluster management tools. I would also talk about when not to use these APIs and what's planned in the near future to handle specific operational use cases.",
          "onhold": false,
          "bio": "Anshum Gupta is a Lucene\/Solr committer and PMC member with over 10 years of experience with search. He is a part of the search team at IBM Watson, where he works on extending the limits and improving SolrCloud. Prior to this, he was a part of the open source team at Lucidworks and also the co-creator of AWS CloudSearch - the first search as a service offering by AWS.\n\nHe has spoken at multiple international conferences, including Apache Big Data NA, Lucene Solr Revolution, and several meetups.",
          "request_id": "b5fdaea0f6ada94c36d68e9",
          "category": "Solr",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.068,
          "notes": "",
          "tags": "",
          "id": "b5fdaea0f6ada94c36d68e9",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Large scale SolrCloud cluster management via APIs",
          "speaker": "Anshum Gupta",
          "accepted": true,
          "submitter": "ce392135cb8ad9557db7c6f52106a011f9dd27815843916acf14c322"
        },
        "room": "d2833fd70409436c6",
        "day": 1479168000,
        "assignee": "b5fdaea0f6ada94c36d68e9",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479226800
      }, {
        "starttime": 1479227400,
        "origin": "d2833fd70409436c6",
        "id": "382474a41d90efdf6",
        "duration": 50,
        "readonly": false,
        "request_id": "382474a41d90efdf6",
        "talk": {
          "description": "Discover a number of Apache projects you may not have heard of and how they can help you process both Clinical and non Clinical data. Apache OODT developed by NASA allows users to ingest and store files and metadata along with process workflows. OODT along with CTakes allows us to extract clinical information from files and then process them and allow end users access to the extracted data. \n\n\n\nWe can then take these sources and manipulate them further creating a highly flexible ETL pipeline offering reliability and scalability. Backed by Apache SOLR users can then interrogate the data via web interfaces and instigate further post processing and investigation.\n\n\n\nOf course you may not have a clinical use case, but the platforms can be repurposed and will allow you to go away and build your own, scalable data pipeline for processing and integstion.",
          "onhold": false,
          "bio": "Tom Barber is the director of Meteorite BI and Spicule BI. A member of the Apache Software Foundation and regular speaker at ApacheCon, Tom has a passion for simplifying technology. The creator of Saiku Analytics and open source stalwart, when not working for NASA, Tom currently deals with Devops and data processing systems for customers and clients, both in the UK, Europe and also North America.",
          "request_id": "e70fdce5dbfc984a89a40f7",
          "category": "Solr",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.0053,
          "notes": "",
          "tags": "",
          "id": "e70fdce5dbfc984a89a40f7",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "ETL pipelines with OODT, Solr and Stuff",
          "speaker": "Tom Barber",
          "accepted": true,
          "submitter": "bda714128d8ffacfe63b9c9995bd86a2593662f7d6367bfa88389d1c"
        },
        "room": "d2833fd70409436c6",
        "day": 1479168000,
        "assignee": "e70fdce5dbfc984a89a40f7",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479230400
      }]
    }]
  }, {
    "id": "031d35442d13f536f",
    "name": "Giralda III\/IV",
    "days": [{
      "day": 1479340800,
      "slots": [{
        "starttime": 1479380400,
        "origin": "031d35442d13f536f",
        "id": "2ee5dcddc315d5c2f",
        "duration": 50,
        "readonly": false,
        "request_id": "2ee5dcddc315d5c2f",
        "talk": {
          "description": "Since April 2016, SICS Swedish ICT has provided Hadoop\/Spark\/Flink\/Kafka\/Zeppelin-as-a-service to researchers in Sweden. We have developed a UI-driven multi-tenant platform (Apache v2 licensed) in which researchers securely develop and run their applications. Applications can be either deployed as jobs (batch or streaming) or written and run directly from Notebooks in Apache Zeppelin. All applications are run on YARN within a security framework built on project-based multi-tenancy. A project is simply a grouping of users and datasets. Datasets are first-class entities that can be securely shared between projects. Our platform also introduces a necessary condition for elasticity: pricing.  Application execution time in YARN is metered and charged to projects, that also have HDFS quotas for disk usage. We also support project-specific Kafka topics that can also be securely shared.",
          "onhold": false,
          "bio": "Jim Dowling is an Associate Professor at the School of Information and Communications Technology in the Department of Software and Computer Systems at KTH Royal Institute of Technology as well as a Senior Researcher at SICS äóñ Swedish ICT. He received his Ph.D. in Distributed Systems from Trinity College Dublin (2005) and worked at MySQL AB (2005-2007). He is a distributed systems researcher and his research interests are in the area of large-scale distributed computer systems. He is lead architect of Hadoop Open Platform-as-a-Service (www.hops.io), a next generation distribution of Hadoop for Humans.",
          "request_id": "0db9f93b1678151f816a289",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.115,
          "notes": "",
          "tags": "",
          "id": "0db9f93b1678151f816a289",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "On-Premise, UI-Driven Hadoop\/Spark\/Flink\/Kafka\/Zeppelin-as-a-service",
          "speaker": "Jim Dowling",
          "accepted": true,
          "submitter": "4e002758f0e43dfcc9a933a1e06bde043b9c3054bfa67456f9f17db2"
        },
        "room": "031d35442d13f536f",
        "day": 1479340800,
        "assignee": "0db9f93b1678151f816a289",
        "title": "15:30 -> 16:20 (50 min)",
        "endtime": 1479383400
      }, {
        "starttime": 1479384000,
        "origin": "031d35442d13f536f",
        "id": "e4e1f5549dee2b916",
        "duration": 50,
        "readonly": false,
        "request_id": "e4e1f5549dee2b916",
        "talk": {
          "description": "This presentation will talk about how to deisgn a highly effective scalable\/performant distributed system to find the identity theft and fraud by mining billions of records related to share holding for a leading financial organization. This will also discuss on how Tera bytes of data can be migrated from Oracle to Hadoop, stored in parquet format, processed in a distributed computing framework with Spark DataFrame and pushed to different service layer (HBase, Impala, Solr, HDFS) depends on the query\/access pattern. This design will also throw light on how the frequent transactions were handled and data were pre-processed end of the day to meet the seconds response time SLA, creating thousands of report by mining millions of record in minutes time.",
          "onhold": false,
          "bio": "Manidipa Mitra heads the Big Data CoE in ValueLabs having extensive experience in building industry specific solution using distributed computing and cloud technologies . Having 16+ years of software industry experience and in-depth knowledge on disruptive-technologies, Cloud and Storage . She is holding dual graduate degree in Physics and Computer science. Manidipa previously Invited as Speaker in Grace Hopper Conference 2013, and presently Chair of Open Source Track in Grace Hopper 2016 India. She has written papers in major forums about her work in distributed computing and machine learning. Manidipa has earlier worked with companies like EMC, Agilet Technologies and Ixia.",
          "request_id": "0ec1861c6d200c23a723e5f",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.5567,
          "notes": "",
          "tags": "",
          "id": "0ec1861c6d200c23a723e5f",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Mining and Identifying security threat using Spark SQL, HBase and Solr",
          "speaker": "Manidipa Mitra",
          "accepted": true,
          "submitter": "8e48c3847ce7def2ac447c94d0826c727c841d1fd6d786a881129f22"
        },
        "room": "031d35442d13f536f",
        "day": 1479340800,
        "assignee": "0ec1861c6d200c23a723e5f",
        "title": "15:30 -> 16:20 (50 min)",
        "endtime": 1479387000
      }, {
        "starttime": 1479387600,
        "origin": "031d35442d13f536f",
        "id": "140d0554ae06711a5",
        "duration": 50,
        "readonly": false,
        "request_id": "140d0554ae06711a5",
        "talk": {
          "description": "While building a massively scalable real time pipeline to collect transaction logs from network traffic, one of the major challenges was performing aggregation on streaming data on the fly. This was needed to compute multiple metrics across various dimensions which help our customer to see near real time views of application delivery and performance. In this talk, learn how we designed our real time pipeline for doing multi-stage aggregation powered by Kafka ,Spark Streaming and ElasticSearch. At InstartLogic we used custom Spark Receiver for Kafka which is used in first stage aggregation. The second stage includes Spark Streaming driven aggregation within given batch window . Final stage aggregation involves custom ElasticSearch plugins to aggregate across Batches. I will cover this  multi-stage aggregation,including optimisation across all stages which is scalable beyond million RPS",
          "onhold": false,
          "bio": "Dibyendu Holds MS in Software Systems and B.Tech in Computer Science having experience in building applications and products leveraging distributed computing and big data technologies. Presently working as Data Platform Engineer at InstartLogic, the world's first endpoint-aware application delivery solution for making web and mobile application fast . Dibyendu has extensive experience building scalable data platform specialised in streaming architecture. As a Spark Contributor he has implemented Receiver based Kafka Spark consumer for Spark Streaming which is now part of spark-packages.org. Dibyendu was previously invited as speaker in major Big Data conferences like HBaseCon, LuceneSolrRevolution and Apache BigData Conference. While not playing with big data, Dibyendu plays with his eleven year old \"big girl\".",
          "request_id": "a076eb832911856f6f75590",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.7563,
          "notes": "",
          "tags": "",
          "id": "a076eb832911856f6f75590",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Real Time Aggregation with Kafka ,Spark Streaming and ElasticSearch , scalable beyond Million RPS",
          "speaker": "Dibyendu Bhattacharya",
          "accepted": true,
          "submitter": "5f1491ce7757b9eb10e66ccf6ac3d4f05766cd0bca588c86d413b271"
        },
        "room": "031d35442d13f536f",
        "day": 1479340800,
        "assignee": "a076eb832911856f6f75590",
        "title": "15:30 -> 16:20 (50 min)",
        "endtime": 1479390600
      }]
    }, {
      "day": 1479254400,
      "slots": [{
        "starttime": 1479294000,
        "origin": "031d35442d13f536f",
        "id": "fa699afe97098d3f4",
        "duration": 50,
        "readonly": false,
        "request_id": "fa699afe97098d3f4",
        "talk": {
          "description": "Common Crawl is non-profit organization which regularily crawls a significant sample of the web and makes the data accessible free charge to everyone interested in running machine-scale analysis on web data.  The presentation will demonstrate how to use the Common Crawl data covering data formats and tools as well as examples and derived datasets.  The monthly crawls are run by Apache Nutch on Apache Hadoop.  Sebastian will also share his experience from running a web-scale crawl on a small budget. ",
          "onhold": false,
          "bio": "Sebastian Nagel works as crawl engineer at Common Crawl, a non-profit organization that makes web data freely accessible to everyone. Prior to joining Common Crawl he implemented search and data quality solutions at  Exorbyte.  Sebastian is a committer and PMC of Apache Nutch, a scalable web crawler, and presented the project  at ApacheCon 2014.\n\n",
          "request_id": "4ced174c443e6ab7afe1221",
          "category": "crawl",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.162,
          "notes": "",
          "tags": "",
          "id": "4ced174c443e6ab7afe1221",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Crawling the Web for Common Crawl",
          "speaker": "Sebastian Nagel",
          "accepted": true,
          "submitter": "2d690d501880c39afb9448b369f3aa2c52c2b252f18bc1e63b992977"
        },
        "room": "031d35442d13f536f",
        "day": 1479254400,
        "assignee": "4ced174c443e6ab7afe1221",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479297000
      }, {
        "starttime": 1479297600,
        "origin": "031d35442d13f536f",
        "id": "ff9839a4aab154fd3",
        "duration": 50,
        "readonly": false,
        "request_id": "ff9839a4aab154fd3",
        "talk": {
          "description": "Few people remember that before spinning off Hadoop and focusing on crawling, Nutch was meant to be an alternative to commercial search engines. What if we tried to do it again today?\n\n\n\nIn this presentation, Sylvain Zimmer will explain how he used projects from the Nutch diaspora like Spark and Elasticsearch to build Common Search, an open source search engine with transparent rankings.\n\n\n\nWe will go over the architecture of large-scale search engines and how it has evolved since the late 90s. Then we will review the tools from the Apache and open source ecosystems that are best suited to solve the many challenges at hand. Finally, we will discuss what lies ahead for Common Search before it can be useful to the general public.",
          "onhold": false,
          "bio": "Sylvain Zimmer is a software developer and longtime free culture advocate. In 2004 he founded Jamendo, the largest Creative Commons music community online. Since 2012, he has been the CTO of Pricing Assistant, a startup specialized in large-scale crawling of E-commerce websites. He is also the founder and main curator of dotConferences, a series of TED-like developer events in Paris. \n\nMore recently, he started Common Search, an ambitious open source project building a transparent search engine for the Web.\n\nBesides being a conference organizer himself, Sylvain has spoken at many events including FluentConf, PyCon France, Paris.py, ParisJS, Wikimania & Creative Commons Summit.",
          "request_id": "56e45299bbd6a37fc9d0cf2",
          "category": "crawl",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.8696,
          "notes": "",
          "tags": "",
          "id": "56e45299bbd6a37fc9d0cf2",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "The original vision of Nutch, 14 years later: building an open source search engine",
          "speaker": "Sylvain Zimmer",
          "accepted": true,
          "submitter": "38552b2971e5e35cd488de27877374e10df5960be1b216e04075fa7b"
        },
        "room": "031d35442d13f536f",
        "day": 1479254400,
        "assignee": "56e45299bbd6a37fc9d0cf2",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479300600
      }, {
        "starttime": 1479301200,
        "origin": "031d35442d13f536f",
        "id": "1a91811691a195704",
        "duration": 50,
        "readonly": false,
        "request_id": "1a91811691a195704",
        "talk": {
          "description": "A web crawler is a bot program that fetches resources from the web for the sake of building applications like search engines, knowledge bases, etc. In this presentation, Karanjeet Singh and Thamme Gowda will describe a new crawler called Sparkler (contraction of Spark-Crawler) that makes use of recent advancements in distributed computing and information retrieval domains by conglomerating various Apache projects like Spark, Kafka, Lucene\/Solr, Tika, and Felix. Sparkler is extensible, highly scalable, and high-performance web crawler that is an evolution of Apache Nutch and runs on Apache Spark Cluster. GitHub Link - https:\/\/github.com\/USCDataScience\/sparkler",
          "onhold": false,
          "bio": "He is pursuing his Master's degree in Computer Science from the University of Southern California (USC). His projects and research are mostly from the area of Information Retrieval and Data Science. He is also affiliated with NASA Jet Propulsion Lab. Prior to this, he was working at Computer Sciences Corporation (CSC) as a web developer for a U.S. based financial firm.",
          "request_id": "07cb7de1c069829a620c09c",
          "category": "Spark",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.7826,
          "notes": "",
          "tags": "",
          "id": "07cb7de1c069829a620c09c",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Sparkler - Crawler on Apache Spark",
          "speaker": "Karanjeet Singh",
          "accepted": true,
          "submitter": "7b27f85dc34ad3676ca8c67024e7d8b01affbd437ba4710e5e5b2a45"
        },
        "room": "031d35442d13f536f",
        "day": 1479254400,
        "assignee": "07cb7de1c069829a620c09c",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479304200
      }, {
        "starttime": 1479310200,
        "origin": "031d35442d13f536f",
        "id": "0f90b7efc06d549b0",
        "duration": 50,
        "readonly": false,
        "request_id": "0f90b7efc06d549b0",
        "talk": {
          "description": "StormCrawler is an open source collection of resources, mostly implemented in Java, for building low-latency, scalable web crawlers on Apache Storm. After a short introduction to Apache Storm and an overview of what StormCrawler provides, we will compare it with similar projects like Apache Nutch and present several real life use cases.  In particular we will see how StormCrawler can be used with ElasticSearch and Kibana for crawling and indexing web pages and also monitor the crawl itself.",
          "onhold": false,
          "bio": "I run DigitalPebble Ltd, a consultancy based in Bristol, UK and specialising in open source solutions for text engineering. My expertise covers web crawling, natural language processing, machine learning and search. I am a committer on Apache Nutch and am also involved in several other open source projects, including StormCrawler and Behemoth. I gave talks at conferences such as ApacheCon, BerlinBuzzwords and LuceneSOLRRevolution.",
          "request_id": "1be99c353e7e73ae4d62afd",
          "category": "crawl",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.4437,
          "notes": "",
          "tags": "",
          "id": "1be99c353e7e73ae4d62afd",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Low latency web crawling on Apache Storm",
          "speaker": "Julien Nioche",
          "accepted": true,
          "submitter": "20cd2a2e8622990a9408e094aa44b420ec4982c900004460101c435d"
        },
        "room": "031d35442d13f536f",
        "day": 1479254400,
        "assignee": "1be99c353e7e73ae4d62afd",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479313200
      }, {
        "starttime": 1479313800,
        "origin": "031d35442d13f536f",
        "id": "c0f27b7c40655fecd",
        "duration": 50,
        "readonly": false,
        "request_id": "c0f27b7c40655fecd",
        "talk": {
          "description": "Common Search is building an open source search engine based on Common Crawl's monthly dumps of several billion webpages. Ranking every URL on the Web in a transparent and reproducible way is core to the project.\n\n\n\nIn this presentation, Sylvain Zimmer will explain why Spark is a great match for the job, how the current ranking pipeline works and what challenges it faces to grow in scale and complexity, in order to improve the quality of search results.\n\n\n\nSpecifically, we will dive in the new Spark 2.0 features that made it practical to compute PageRank from Python on every URL found in Common Crawl, and show how anyone can reproduce and tweak the results on their cloud servers.",
          "onhold": false,
          "bio": "Sylvain Zimmer is a software developer and longtime free culture advocate. In 2004 he founded Jamendo, the largest Creative Commons music community online. Since 2012, he has been the CTO of Pricing Assistant, a startup specialized in large-scale crawling of E-commerce websites. He is also the founder and main curator of dotConferences, a series of TED-like developer events in Paris.\n\nMore recently, he started Common Search, an ambitious open source project building a transparent search engine for the Web.\n\nBesides being a conference organizer himself, Sylvain has spoken at many events including FluentConf, PyCon France, Paris.py, ParisJS, Wikimania & Creative Commons Summit.",
          "request_id": "0c3490da30c9abcaab16e19",
          "category": "Spark",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.8641,
          "notes": "",
          "tags": "",
          "id": "0c3490da30c9abcaab16e19",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Ranking the Web with Spark",
          "speaker": "Sylvain Zimmer",
          "accepted": true,
          "submitter": "38552b2971e5e35cd488de27877374e10df5960be1b216e04075fa7b"
        },
        "room": "031d35442d13f536f",
        "day": 1479254400,
        "assignee": "0c3490da30c9abcaab16e19",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479316800
      }]
    }, {
      "day": 1479168000,
      "slots": [{
        "starttime": 1479207600,
        "origin": "031d35442d13f536f",
        "id": "674d5183477be89d1",
        "duration": 50,
        "readonly": false,
        "request_id": "674d5183477be89d1",
        "talk": {
          "description": "Nowadays, the main burden of BigData adoption is clearly the integration of new infrastructure and technologies with legacy systems, especially when dealing with data ingestion.\n\n\n\nIn this talk, KEEDIO will present the details of the aforementioned BigData architecture, deployed in a hybrid infrastructure for a rising bank in Spain, in order to provide added value to its customers. This success story has been possible by means of custom analytics built on top of several components of the Apache Stack.\n\n\n\nThe main and most interesting issues of this deployment will be explained as well as the their solutions based on tools like Apache NiFi, Apache Spark, Apache Mesos and Apache Zeppelin. Thus, the complete ingestion architecture will be outlined, as well as data consolidation and processing.\n\n\n\nFinally, the low latency online data exploitation architecture will be explained.",
          "onhold": false,
          "bio": "Luca has been working on Big Data project for major Spanish corporations for the last four years. He now serves as the CTO of KEEDIO, a young spanish startup focused in solving BigData problems in banking environments. \n\nHe holds a master degree in computer engineering at the university of Pisa, Italy.\n\n\n\nPrevious talks:\n\nSpark Summit 2013\n\nBigData Spain 2013 ",
          "request_id": "bac08dada05f78213e55299",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.7227,
          "notes": "",
          "tags": "",
          "id": "bac08dada05f78213e55299",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "The role of Apache BigData Stack in finance: a real-world experience on providing added value to online customers",
          "speaker": "Luca Rosellini",
          "accepted": true,
          "submitter": "fd6b08486f8b39f661d7db72a7c1c4709d8394fbc784e42341e28dc3"
        },
        "room": "031d35442d13f536f",
        "day": 1479168000,
        "assignee": "bac08dada05f78213e55299",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479210600
      }, {
        "starttime": 1479211200,
        "origin": "031d35442d13f536f",
        "id": "dd3745dc5edfab42b",
        "duration": 50,
        "readonly": false,
        "request_id": "dd3745dc5edfab42b",
        "talk": {
          "description": "The collection, documentation, management and analysis of big data associated with non-motorized travel has not attracted enough attentions. This may not conform to the trend that cycling, walking and jogging are strongly advocated by governments to build low-carbon cities and also to improve peopleäó»s health conditions. This session will share the experience that quantify and characterize the non-motorized travel by means of tempo-spatial analysis. The data used in this case is captured from a famous online community for running amateurs sharing their activities. Around 0.5 million running and cycling records from 0.3 million people in Beijing are analyzed with machine learning and data science methodology in this case study. Spark ML with random forest algorithm, and grid search of the parameters selection have been used on the prediction upon weather, AQI and time.",
          "onhold": false,
          "bio": "Henry Zeng, senior  architect of big data and analytics based in IBM China Development Lab.Henry has more than 10 years experience on data management related products, system and applications development and architecturing, he has two bookspublished in this area. He is now the solution architect responsible for the big data and data science product and solutions cooperation with partners and customers in IBM Greater China Group.",
          "request_id": "1dbafe52835e2a2c86d0114",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.4349,
          "notes": "",
          "tags": "",
          "id": "1dbafe52835e2a2c86d0114",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Data Science with Spark and Case Study with Non-motorized Travel Social Data for the Public",
          "speaker": "yonghua zeng",
          "accepted": true,
          "submitter": "07a3991781b445edc78281beca6dd88f6ce559c43675fbad100f35ab"
        },
        "room": "031d35442d13f536f",
        "day": 1479168000,
        "assignee": "1dbafe52835e2a2c86d0114",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479214200
      }, {
        "starttime": 1479214800,
        "origin": "031d35442d13f536f",
        "id": "70c4ac77f8722aac0",
        "duration": 50,
        "readonly": false,
        "request_id": "70c4ac77f8722aac0",
        "talk": {
          "description": "Building data pipelines is pretty hard! Building a multi-datacenter active-active real time data pipeline for multiple classes of data with different durability, latency and availability guarantees is much harder. \n\n\n\nReal time infrastructure powers critical pieces of Uber (think Surge) and in this talk we will discuss our architecture, technical challenges, learnings and how a blend of open source infrastructure (Apache Kafka and Samza) and in-house technologies have helped Uber scale.\n\n",
          "onhold": false,
          "bio": "Aditya manages the Streaming Data platform team at Uber. Powering pub-sub style event transport, streaming\/batch analytics and ingestion are some examples of use-cases. Previously at LinkedIn, he managed the Apache Kafka engineering team and was one of the earliest members of the Espresso team (distributed storage).\n\n\n\nPreviously spoken at Apache: Big Data Vancouver.",
          "request_id": "3b80de1d829042413b5c91f",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.8283,
          "notes": "",
          "tags": "",
          "id": "3b80de1d829042413b5c91f",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Uber - Your Realtime data pipeline is arriving now!",
          "speaker": "Aditya Auradkar",
          "accepted": true,
          "submitter": "eaf29bcde830925b22464fa29fc5a150307da66fc70b065ab3578d70"
        },
        "room": "031d35442d13f536f",
        "day": 1479168000,
        "assignee": "3b80de1d829042413b5c91f",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479217800
      }, {
        "starttime": 1479223800,
        "origin": "031d35442d13f536f",
        "id": "002f9e50de9db3c99",
        "duration": 50,
        "readonly": false,
        "request_id": "002f9e50de9db3c99",
        "talk": {
          "description": "Identity Theft is no longer just a consumeräó»s problem. Attackers are now targeting Enterprises for bigger financial gains and greater damage not just to the organizationäó»s infrastructure but more importantly to its corporate image.  \n\n\n\nWhile Enterprise Identity Theft Analytics Tools do exist, most organizations find it economically prohibitive to invest in expensive proprietary software. In this session, Seshika will show how a comprehensive Identity Theft Analytics Solution can be built using Open Source Technologies. She will demonstrate how Big Data Analytics can be used to safeguard any Enterprise by covering the 4 Aäó»s of Identity Analytics \n\näó¢\tAuthentication Analytics\n\näó¢\tAuthorization Analytics\n\näó¢\tAudit Trail Analytics\n\näó¢\tAdaptive Analytics",
          "onhold": false,
          "bio": "Seshika is a Senior Technical Lead at WSO2 and focuses on the applications of WSO2äó»s middleware platform in Financial Markets. Throughout her career, she has had extensive experience in providing technology for Stock Exchanges, Regulators and Investment Banks from across the globe. Her current area of interest is in Real time anomaly detection and its usage in e-commerce.\n\nShe holds a BSc (Hons) in Computer Science from the University of Colombo, Sri Lanka and an MSc in Finance from the London School of Economics, UK and is a British Chevening Scholar.\n\nShe has spoken at In Memory Computing Summit 2015 in San Francisco, Apache Big Data Europe 2015, Big Data Tech Con 2015 in Chicago and many WSO2 Conferences over the years.\n\n",
          "request_id": "39289cafcd53afd31c7378c",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.063,
          "notes": "",
          "tags": "",
          "id": "39289cafcd53afd31c7378c",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Fighting Identity Theft: Big Data Analytics to the Rescue",
          "speaker": "Seshika Fernando",
          "accepted": true,
          "submitter": "0cba65641f19db7f1834e7c180a006bac87f909ae83fc9033cd89871"
        },
        "room": "031d35442d13f536f",
        "day": 1479168000,
        "assignee": "39289cafcd53afd31c7378c",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479226800
      }, {
        "starttime": 1479227400,
        "origin": "031d35442d13f536f",
        "id": "3bf1282e24c7e06a8",
        "duration": 50,
        "readonly": false,
        "request_id": "3bf1282e24c7e06a8",
        "talk": {
          "description": "This session will talk about how a Digital Health Care Mgmt platform can be built (using different open source technologies like Kafka,Spark Streaming,HBase,Hive,pySpark,Mirth)to collect patient data,clinical data(HL7 data),claims data,real-time wearables data and create a 360 view\/insights for a patient's health risk and conditions. Also it will talk about how to built a generic platform(by scraping blogs,message board, articles, using an open source called Scrapy.Ingesting fb,twitter data,store,analyse,index,built social-sentiments,create word cloud,segment messages using open source like spark,HBase,Hive,python,Solr)to find out a Key Opinion Leader for a particular disease discussion in social media and how to provide insights\/social-sentiments and search capabilities on different medicines used for particular disease\/treatment to get feedback on medicines or for research purpose.. ",
          "onhold": false,
          "bio": "Manidipa Mitra heads the Big Data CoE in ValueLabs having extensive experience in building industry specific solution using distributed computing and cloud technologies  . Having 16+ years of software industry experience and in-depth knowledge on disruptive-technologies, Cloud and Storage . She is holding dual graduate degree in Physics and Computer science. Manidipa previously Invited as Speaker in Grace Hopper Conference 2013, and presently Chair of Open Source Track in Grace Hopper 2016 India. She has written papers in major forums about her work in distributed computing and machine learning. Manidipa has earlier worked with companies like EMC, Agilet Technologies and Ixia. ",
          "request_id": "8514fc448107f04015ed45e",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.5783,
          "notes": "",
          "tags": "",
          "id": "8514fc448107f04015ed45e",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "How Big Data\/IoT leverage the power of OpenSource to solve Health Care use cases",
          "speaker": "Manidipa Mitra",
          "accepted": true,
          "submitter": "8e48c3847ce7def2ac447c94d0826c727c841d1fd6d786a881129f22"
        },
        "room": "031d35442d13f536f",
        "day": 1479168000,
        "assignee": "8514fc448107f04015ed45e",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479230400
      }]
    }]
  }, {
    "id": "2077d8018648b31da",
    "name": "Carmona",
    "days": [{
      "day": 1479340800,
      "slots": [{
        "starttime": 1479380400,
        "origin": "2077d8018648b31da",
        "id": "edcc238f7991acd9c",
        "duration": 50,
        "readonly": false,
        "request_id": "edcc238f7991acd9c",
        "talk": {
          "description": "Apache Hadoop is used to run jobs that execute tasks over multiple machines with complex dependencies between tasks. And at scale, there can be 10äó»s to 1000äó»s of tasks running over 100's to 1000äó»s of machines which increases the challenge of making sense of their performance. Pipelines of such jobs that logically run a business workflow add another level of complexity. No wonder that the question of why Hadoop jobs run slower than expected remains a perennial source of grief for developers. In this talk, we will draw on our experience in debugging and analyzing Hadoop jobs to describe some methodical approaches to this and present current and new tracing and tooling ideas that can help semi-automate parts of this difficult problem.",
          "onhold": false,
          "bio": "Bikas is an active Apache community member and has contributed to the Apache Hadoop and Tez projects and focuses mainly on the distributed compute stack on Hadoop. He works for Hortonworks, a company that supports an open source based Apache Hadoop distribution. Bikas has spoken widely on the Hadoop compute stack over the last few years at conferences in America, SIGMOD 2015 Australia, ApacheCon Europe and Big Data Technology Conference in China, among others.",
          "request_id": "bb675945b0215743f96574d",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.3652,
          "notes": "",
          "tags": "",
          "id": "bb675945b0215743f96574d",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Why is my Hadoop cluster slow?",
          "speaker": "Bikas Saha",
          "accepted": true,
          "submitter": "63f09c3870c213cb9fbd6a91b7b0503668d1b56fe1c3d4afd116014f"
        },
        "room": "2077d8018648b31da",
        "day": 1479340800,
        "assignee": "bb675945b0215743f96574d",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479383400
      }, {
        "starttime": 1479384000,
        "origin": "2077d8018648b31da",
        "id": "7304ab51f46d283e0",
        "duration": 50,
        "readonly": false,
        "request_id": "7304ab51f46d283e0",
        "talk": {
          "description": "Whenever you work with data, sooner or later you stumble across the definition of your workflows. At what point should you process your customeräó»s data? What subsequent steps are necessary? And what went wrong with your data processing last Saturday night?\n\n\n\nAt Blue Yonder we use Apache Airflow to solve these problems. It can be extended with new functionality by developing plugins in Python. With Airflow, we define workflows as directed acyclic graphs and get a shiny UI for free. Airflow comes with some task operators which can be used out of the box to complete certain tasks. For more specific cases, you can also develop new operators in your plugin.\n\n\n\nThis talk will explain the concepts behind Airflow, demonstrating how to define your own workflows and how to extend the functionality. Youäó»ll also get to hea about our experiences using  this tool in real-world scenarios.",
          "onhold": false,
          "bio": "Christian is a Software Developer from Karlsruhe, Germany. He has studied Computer Science at TU Darmstadt. Currently he is working on big data applications at Blue Yonder, enjoying the challenges at the intersection between software engineering and data science. ",
          "request_id": "d338e6564404268eca1590e",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.7466,
          "notes": "",
          "tags": "",
          "id": "d338e6564404268eca1590e",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Get in control of your workflows with Apache Airflow",
          "speaker": "Christian Trebing",
          "accepted": true,
          "submitter": "af802199a11431d50e3f9abd96821c763fac8b6801f17dcae060cbbe"
        },
        "room": "2077d8018648b31da",
        "day": 1479340800,
        "assignee": "d338e6564404268eca1590e",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479387000
      }, {
        "starttime": 1479387600,
        "origin": "2077d8018648b31da",
        "id": "eb9fcd54a7a893319",
        "duration": 50,
        "readonly": false,
        "request_id": "eb9fcd54a7a893319",
        "talk": {
          "description": "Big Data analytics is becoming more and more popular as the query response times improve. We'll look at building and deploying a fully operational and highly scalable Apache Bigtop based Big Data Analytics platform with no code. \n\nIn this talk we'll utilise the power of the open source Juju application modelling platform to deploy our software and configure it for us. We'll also discuss deployment options, scalability and resilliency allowing users to get the most from the data.",
          "onhold": false,
          "bio": "Tom Barber is the director of Meteorite BI and Spicule BI. A member of the Apache Software Foundation and regular speaker at ApacheCon, Tom has a passion for simplifying technology. The creator of Saiku Analytics and open source stalwart, when not working for NASA, Tom currently deals with Devops and data processing systems for customers and clients, both in the UK, Europe and also North America.",
          "request_id": "d77a036bc09e54c63e026de",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.9945,
          "notes": "",
          "tags": "",
          "id": "d77a036bc09e54c63e026de",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Highly Scalable Big Data Analytics with Apache Drill",
          "speaker": "Tom Barber",
          "accepted": true,
          "submitter": "bda714128d8ffacfe63b9c9995bd86a2593662f7d6367bfa88389d1c"
        },
        "room": "2077d8018648b31da",
        "day": 1479340800,
        "assignee": "d77a036bc09e54c63e026de",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479390600
      }]
    }, {
      "day": 1479254400,
      "slots": [{
        "starttime": 1479294000,
        "origin": "2077d8018648b31da",
        "id": "838b18f2a87153f64",
        "duration": 50,
        "readonly": false,
        "request_id": "838b18f2a87153f64",
        "talk": {
          "description": "NoSQL databases are critical in building Big Data applications. Apache HBase, one of the most popular NoSQL databases, is used by Facebook, Apple, eBay and hundreds of other enterprises to store, analyze and profit from their petabyte-scale volume of data. This talk will discuss\n\n- motivation behind NoSql databases\n\n- basic architecture of a popular NoSql system, Apache HBase\n\n- some commonly seen big data usage patterns in industry, and when & how to use Apache HBase (or other better suited NoSQL database).",
          "onhold": false,
          "bio": "Apekshit Sharma (Appy) is a Software Engineer at Cloudera, and contributor of Apache HBase. Prior, he was at Google building backend infrastructure using Map-Reduce, Bigtable & Millwheel. He earned his B.Tech in Computer Science from Indian Institute of Technology, Bombay. Currently he is working on performance framework and dynamic configuration framework in Apache HBase. He has also given tutorials in NoSqlNow! and JavaOne conferences.",
          "request_id": "5fa1dd0e1b9f166e6435029",
          "category": "intro",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.2545,
          "notes": "",
          "tags": "",
          "id": "5fa1dd0e1b9f166e6435029",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Apache HBase: Overview and Use Cases",
          "speaker": "Apekshit Sharma",
          "accepted": true,
          "submitter": "584f0f72cc88156f4cebd173761af1cbf0ff152fd2f1028a060fa1e7"
        },
        "room": "2077d8018648b31da",
        "day": 1479254400,
        "assignee": "5fa1dd0e1b9f166e6435029",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479297000
      }, {
        "starttime": 1479297600,
        "origin": "2077d8018648b31da",
        "id": "2ded7d7d01931b240",
        "duration": 50,
        "readonly": false,
        "request_id": "2ded7d7d01931b240",
        "talk": {
          "description": "Stream data processing is becoming increasingly important to support business needs for faster time to insight and action with growing volume of information from more sources. Apache Apex (http:\/\/apex.apache.org\/) is a unified big data in motion processing platform for the Apache Hadoop ecosystem. Apex supports demanding use cases with:\n\n\n\n* Architecture for high throughput, low latency and exactly-once processing semantics.\n\n* Rich library of building blocks including connectors for Kafka, Files, Cassandra, HBase and many more \n\n* Java based with unobtrusive API to build real-time and batch applications and implement custom business logic.\n\n* Advanced engine features for auto-scaling, dynamic changes, compute locality.\n\n\n\nApex was developed since 2012 and is used in production in various industries like online advertising, Internet of Things (IoT) and financial services.",
          "onhold": false,
          "bio": "Thomas is Apache Apex PMC member and architect\/co-founder at DataTorrent. He has developed distributed systems, middleware and web applications since 1997. Prior to DataTorrent he was in the Hadoop Team at Yahoo! and contributed to projects like Pig and Hive and migration to next generation Hadoop 2.x.\n\n",
          "request_id": "554ed31a539db8123a783f0",
          "category": "intro",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.5386,
          "notes": "",
          "tags": "",
          "id": "554ed31a539db8123a783f0",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Introducing Apache Apex: Next Gen Big Data Processing on Hadoop",
          "speaker": "Thomas Weise",
          "accepted": true,
          "submitter": "10fc0d8b1ea7e46f556f1e253fa7d90827a3538cc8918b729eae3f73"
        },
        "room": "2077d8018648b31da",
        "day": 1479254400,
        "assignee": "554ed31a539db8123a783f0",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479300600
      }, {
        "starttime": 1479301200,
        "origin": "2077d8018648b31da",
        "id": "f821348b53476a053",
        "duration": 50,
        "readonly": false,
        "request_id": "f821348b53476a053",
        "talk": {
          "description": "Querying information over TBs of data where no one can see what you query or the responses obtained? It sounds like science fiction, but it is actually the science of Private Information Retrieval (PIR). This talk will introduce Apache Pirk - a new incubating Apache project designed to provide a framework for scalable, distributed PIR. We will discuss the motivation for Apache Pirk, its distributed implementations in platforms such as Spark and Storm, itäó»s current algorithms, the power of homomorphic encryption, and take a look at the path forward.",
          "onhold": false,
          "bio": "Ellison Anne Williams is a creator and PMC member of Apache Pirk, a pure mathematician by training, and a practical computer scientist in real life. Her passion is doing cool stuff with massive amounts of data. ",
          "request_id": "5da82aca7b0845cf794ca32",
          "category": "intro",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.1569,
          "notes": "",
          "tags": "",
          "id": "5da82aca7b0845cf794ca32",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Scalable Private Information Retrieval äóî Introducing Apache Pirk (incubating)",
          "speaker": "Ellison Anne Williams",
          "accepted": true,
          "submitter": "90bc04f6bc0ac0787a13b38fa93c0e8beaa8357f40a5128f77d96814"
        },
        "room": "2077d8018648b31da",
        "day": 1479254400,
        "assignee": "5da82aca7b0845cf794ca32",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479304200
      }, {
        "starttime": 1479310200,
        "origin": "2077d8018648b31da",
        "id": "bca3ef1cd315dae0f",
        "duration": 50,
        "readonly": false,
        "request_id": "bca3ef1cd315dae0f",
        "talk": {
          "description": "Take a journey together to see how Apache Zeppelin started, how Apache Zeppelin helps your data science lifecycle, how Apache Zeppelin became popular TLP project. We'll also see how community focus has been changed, from basic notebook feature, spark integration to advanced features like multi-tenancy. Lee moon soo will explain value of Apache Zeppelin with some key use case scenario demo. Also we'll see eco-system around it - How various projects and companies are using Apache Zeppelin in their product and services in many different ways. \n\nFinally, we'll discuss about Apache Zeppelin's future roadmap with some challenges that community have.",
          "onhold": false,
          "bio": "Moon soo Lee is a creator for Apache Zeppelin and a Co-Founder, CTO at NFLabs. For past few years he has been working on bootstrapping Zeppelin project and itäó»s community. His recent focus is growing Zeppelin community and getting adoptions. ",
          "request_id": "83adaa355df9452022bfa77",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.409,
          "notes": "",
          "tags": "",
          "id": "83adaa355df9452022bfa77",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Your datascience journey with Apache Zeppelin",
          "speaker": "Moon soo Lee",
          "accepted": true,
          "submitter": "fa93c91a88878e259f86c0a153097312a0f9c39956e32ca51aae50ca"
        },
        "room": "2077d8018648b31da",
        "day": 1479254400,
        "assignee": "83adaa355df9452022bfa77",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479313200
      }, {
        "starttime": 1479313800,
        "origin": "2077d8018648b31da",
        "id": "aec64305e0a495038",
        "duration": 50,
        "readonly": false,
        "request_id": "aec64305e0a495038",
        "talk": {
          "description": "In those days, we are generating enormous amount of data. Biggest challenge is hidden in transformation of raw data to knowledge. We would like to take you on a short travel and show our approach for conversion from non-structured world of microservices to the world with Avro schemas inside our data pipelines.\n\nAvro is well known format for storing and online processing information of any kind. What are key features of this format? What are the common problems? Where you can meet pitfails? How this influences our Big Data ecosystem?\n\nWhole story will be covered by examples from real life implementation.",
          "onhold": false,
          "bio": "Works in Allegro Group as a senior data engineer. From the beginning he is related with building and maintaining of Hadoop infrastructure within Allegro Group. Previously he was responsible for maintaining large scale database systems.\n\nPassionate about new technologies and cycling.",
          "request_id": "5471152b2bc211dd4d05b21",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.7769,
          "notes": "",
          "tags": "",
          "id": "5471152b2bc211dd4d05b21",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Avro: Travel across (r)evolution",
          "speaker": "Arek Osinski",
          "accepted": true,
          "submitter": "7bcf54476f0252d425ca5a4f6b7063a45331a6993b189cf61255c4ae"
        },
        "room": "2077d8018648b31da",
        "day": 1479254400,
        "assignee": "5471152b2bc211dd4d05b21",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479316800
      }]
    }, {
      "day": 1479168000,
      "slots": [{
        "starttime": 1479207600,
        "origin": "2077d8018648b31da",
        "id": "0ed145d3e3cd8fb3d",
        "duration": 50,
        "readonly": false,
        "request_id": "0ed145d3e3cd8fb3d",
        "talk": {
          "description": "Apache SIS is a library for helping developers to create their own geospatial application. SIS follows closely international standards published jointly by the Open Geospatial Consortium (OGC) and the International Organization for Standardization (ISO). In this talk we will show how SIS provides a unified metadata model based on ISO 19115 standard for summarizing the content of some file formats used for earth observation: GeoTIFF, NetCDF, Landsat 8 and MODIS. We will show how to get the Coordinate Reference System (CRS) from those file formats or from other sources like Well Known Text (WKT) 2 or registry maintained by authorities, and how to use those CRS for coordinate operations. We will present new issues to take in account when applying those tools to extra-terrestrial bodies like Mars or asteroids. Finally we will present next developments proposed for Apache SIS.",
          "onhold": false,
          "bio": "I hold a Ph.D thesis in oceanography, but have continuously developed tools for helping analysis work. I used C\/C++ before to switch to Java in 1997. I develop geospatial libraries since that time, initially as a personal project then as a GeoTools contributor until 2008. I'm now contributing to Apache SIS since 2013. I attend to Open Geospatial Consortium (OGC) meetings about twice per year in the hope to follow closely standard developments and improve Apache SIS conformance to those standards. I work in a small IT services company (Geomatys) specialized in development of geoportals. Geomatys is an OGC member and develop a stack of open source software for spatial applications, with Apache SIS as the foundation to which Geomatys contributes actively.",
          "request_id": "dae7a8a33306500aee49a34",
          "category": "Geo",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.9223,
          "notes": "",
          "tags": "",
          "id": "dae7a8a33306500aee49a34",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Geospatial track: Apache SIS for Earth observation and beyond",
          "speaker": "Martin Desruisseaux",
          "accepted": true,
          "submitter": "fc26b9424f0eb8144044ef0c266577a002f336735e18db7ad45da033"
        },
        "room": "2077d8018648b31da",
        "day": 1479168000,
        "assignee": "dae7a8a33306500aee49a34",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479210600
      }, {
        "starttime": 1479211200,
        "origin": "2077d8018648b31da",
        "id": "cf5b3f599b3dc8d16",
        "duration": 50,
        "readonly": false,
        "request_id": "cf5b3f599b3dc8d16",
        "talk": {
          "description": "A number of technologies have evolved around big data, in particular products from the Apache community such as Hadoop, Storm, Spark, Hive, or Cassandra. The geospatial community has developed a range of standards to handle geospatial data in an efficient way. Most of these standards are produced by the Open Geospatial Consortium (OGC) and implemented in the form of domain-agnostic data models and Web services. With the emerging demand for streamlined APIs, new questions emerge how access to Big Data in the geospatial community can be handled most efficiently, how existing standards serve these new demands and implementation realities with distributed Big Data repositories operated e.g. by the various space agencies. This presentation should stimulate the discussion of geospatial Big Data handling in standardized environments and explore the role of products from the Apache community.",
          "onhold": false,
          "bio": "Dr. Ingo Simonis is director of interoperability programs and science at the Open Geospatial Consortium (OGC), an international consortium of more than 525 companies, government agencies, research organizations, and universities participating in a consensus process to develop publicly available geospatial standards. As lead architect of OGCäó»s prototyping and exploration program, he supervises the technical experiments and enhancements of geospatial data processing approaches and best practices.",
          "request_id": "90d2ecc1315eff75b1077dd",
          "category": "Geo",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.0832,
          "notes": "",
          "tags": "",
          "id": "90d2ecc1315eff75b1077dd",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Geospatial Big Data: Software Architectures and the Role of APIs in Standardized Environments",
          "speaker": "Ingo Simonis",
          "accepted": true,
          "submitter": "0ecb3af01f754d50ce5dcd2c8ffac41ac88f2df9d2411833c866d9a4"
        },
        "room": "2077d8018648b31da",
        "day": 1479168000,
        "assignee": "90d2ecc1315eff75b1077dd",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479214200
      }, {
        "starttime": 1479214800,
        "origin": "2077d8018648b31da",
        "id": "71a8502dab5dc14a8",
        "duration": 50,
        "readonly": false,
        "request_id": "71a8502dab5dc14a8",
        "talk": {
          "description": "indoo.rs enables location based services for indoor applications. With indoo.rs, developers can add new features to their products, including having locations trigger events, track assets, showing closest routes to other places. For this, we use WiFi\/beacon radio infrastructure, mobile devices and our cloud which produce lots of geospatial time series data. The real-time indoor navigation fuses independent movement from custom 9D sensor fusion and position estimates obtained by comparing current signal readings to a reference map. This talk will discuss how we create and maintain these maps in our big data machine learning system which leverages crowd data through Kafka and Spark to run SLAM and context aware algorithms to create high quality trajectories. In addition to use in reference maps, these trajectories provide an additional input for our interactive analytics.",
          "onhold": false,
          "bio": "Thomas is the CRO of indoo.rs and leads its research efforts since 2012. Earlier, he did his PhD in particle physics at Stockholm University for the AMANDA\/IceCube neutrino telescopes, and worked as a postdoctoral researcher at University of Bergen for the ATLAS experiment at the LHC. At indoors he does modeling, statistics, data science, machine learning, and new algorithms for mobile indoor navigation. He has given numerous public talks for business, academia and general public. At the moment,  he spends most of his time on two projects: indoor radio signal propagation modeling and crowd data analytics with SLAM algorithms.",
          "request_id": "ce50cb791780d14d4c96408",
          "category": "Geo",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.8947,
          "notes": "",
          "tags": "",
          "id": "ce50cb791780d14d4c96408",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Geospatial track: Crowd learning for indoor navigation",
          "speaker": "Thomas Burgess",
          "accepted": true,
          "submitter": "ab5ffa7e38c5c34bf643fee0bae48c5a3b97743e6941566b8c4fab0f"
        },
        "room": "2077d8018648b31da",
        "day": 1479168000,
        "assignee": "ce50cb791780d14d4c96408",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479217800
      }, {
        "starttime": 1479223800,
        "origin": "2077d8018648b31da",
        "id": "fe2f7638f1cffffdd",
        "duration": 50,
        "readonly": false,
        "request_id": "fe2f7638f1cffffdd",
        "talk": {
          "description": "In my group at Microsoft, we have worked with the United Nations, Guide Dogs for the Blind in the UK, several automotive companies, and Strí_er on a number of projects involving high scale geospatial data.\n\n\n\nIn this talk, I'll share some of the best practices and patterns that have come out of those experiences: best practices for storing and indexing geospatial data at scale, incremental ingestion and slice processing of the data, and efficiently building and presenting progressive levels of detail.\n\n\n\nThe audience will walk away with an understanding of how to efficiently summarize data over a geographic area, general methods for doing ingestion with Apache Kafka (or other event ingestion systems), and incremental updates to large scale datasets with Apache Spark, and best practices around visualizing this data on the frontend.",
          "onhold": false,
          "bio": "Tim is a Principal Software Engineer at Microsoft and works with customers and partners to help them utilize open source platforms on Microsoftäó»s Azure cloud.  He has a particular focus on big data, and, in particular, processing large scale geospatial data.  His project experience in this space several global car manufacturers, outdoor display advertiser Stroeer, several Internet of Things startups, and detecting humanitarian crisis situations in conjunction with the United Nations.\n\n\n\nBefore Microsoft, Tim was also one of the key early members of the Android team at Google, and worked on the Internet of Things before it was the Internet of Things at Nest Labs (acquired by Google). \n\n\n\nHe has previously spoken at a variety of developer conferences, including JSConf.eu, LXJS, íÖredev, and QCon London.\n\n\n\nMicrosoft will pay for all travel to\/from and at the conference.",
          "request_id": "90362e34ae93b78f8edd815",
          "category": "geo",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.3431,
          "notes": "",
          "tags": "",
          "id": "90362e34ae93b78f8edd815",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Processing Planetary Sized Datasets",
          "speaker": "Tim Park",
          "accepted": true,
          "submitter": "02f577bc47b13ed22a8aeec429695de944579170d62037a281d0b1b5"
        },
        "room": "2077d8018648b31da",
        "day": 1479168000,
        "assignee": "90362e34ae93b78f8edd815",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479226800
      }, {
        "starttime": 1479227400,
        "origin": "2077d8018648b31da",
        "id": "3390ed59ccfa3a1a2",
        "duration": 50,
        "readonly": false,
        "request_id": "3390ed59ccfa3a1a2",
        "room": "2077d8018648b31da",
        "day": 1479168000,
        "assignee": null,
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479230400
      }]
    }]
  }, {
    "id": "90ab76991543bd7a2",
    "name": "Nervion\/Arenal1",
    "days": [{
      "day": 1479340800,
      "slots": [{
        "starttime": 1479380400,
        "origin": "90ab76991543bd7a2",
        "id": "dde19bfce667ea967",
        "duration": 50,
        "readonly": false,
        "request_id": "dde19bfce667ea967",
        "talk": {
          "description": "Apache Parquet brings the advantages of compressed, efficient columnar data representation available to any project in the Hadoop ecosystem. Apache Parquet is built from the ground up with complex nested data structures in mind, and uses the record shredding and assembly algorithm described in the Dremel paper. We believe this approach is superior to simple flattening of nested name spaces. Apache Parquet is built to support very efficient compression and encoding schemes. Multiple projects have demonstrated the performance impact of applying the right compression and encoding scheme to the data. Apache Parquet allows compression schemes to be specified on a per-column level, and is future-proofed to allow adding more encodings as they are invented and implemented.\n\n\n\nThis talk highlights the internal implementation of Apache Parquet.",
          "onhold": false,
          "bio": "Ranganathan has eleven plus years of experience of developing awesome products and loves to works on full stack - from front end, to backend and scale. He works for ThoughtWorks as Technology Lead. He is a Microsoft MVP in Data Platform. He runs the one of the top technology meetups in Hyderabad - Hyderabad Scalability Meetup. He is very interested in exploring Big data technologies and a regular speaker.",
          "request_id": "5c29786468e4476d454a73f",
          "category": "Column",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.2357,
          "notes": "",
          "tags": "",
          "id": "5c29786468e4476d454a73f",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Efficient Columnar Storage with Apache Parquet",
          "speaker": "Ranganathan Balashanmugam",
          "accepted": true,
          "submitter": "d4834659593994112795259e450af7e80f5259e7c52f4d5f33fb92a6"
        },
        "room": "90ab76991543bd7a2",
        "day": 1479340800,
        "assignee": "5c29786468e4476d454a73f",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479383400
      }, {
        "starttime": 1479384000,
        "origin": "90ab76991543bd7a2",
        "id": "fc2d5c3ecdbe4e215",
        "duration": 50,
        "readonly": false,
        "request_id": "fc2d5c3ecdbe4e215",
        "talk": {
          "description": "The Hadoop ecosystem has recently made great strides in its real-time access capabilities, narrowing the gap compared to traditional database technologies. With systems like Apache Spark, analysts can now run complex queries or jobs over large datasets within a matter of seconds. With systems like Apache HBase, applications can achieve millisecond-scale random access to arbitrarily-sized datasets. However, gaps remain when scans and random access are both required.\n\n\n\nThis talk will investigate the trade-offs between real-time random access and fast analytic performance from the perspective of storage engine internals. It will also describe Apache Kudu, the new addition to the open source Hadoop ecosystem with out-of-the-box integration with Apache Spark, that fills the gap described above to provide a new option to achieve fast scans and fast random access from a single API.",
          "onhold": false,
          "bio": "Mike Percy is a software engineer at Cloudera and a PMC member on Apache Kudu, an open source distributed column store for the Hadoop ecosystem. He is also a PMC member on Apache Flume. Prior to joining Cloudera, Mike worked at Yahoo! building machine learning infrastructure for Big Data. Mike holds a BSCS from UC Santa Cruz and an MSCS from Stanford.",
          "request_id": "af908f73458520995c3ccac",
          "category": "Column",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.6226,
          "notes": "",
          "tags": "",
          "id": "af908f73458520995c3ccac",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Apache Kudu: A distributed, columnar data store for fast analytics",
          "speaker": "Mike Percy",
          "accepted": true,
          "submitter": "1ed9d370f520885ec1e5cccaa810a283f97a56bd69adcca70702e07c"
        },
        "room": "90ab76991543bd7a2",
        "day": 1479340800,
        "assignee": "af908f73458520995c3ccac",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479387000
      }, {
        "starttime": 1479387600,
        "origin": "90ab76991543bd7a2",
        "id": "eec3b00a957a9c6b2",
        "duration": 50,
        "readonly": false,
        "request_id": "eec3b00a957a9c6b2",
        "talk": {
          "description": "Apache Parquet is among the most commonly used column-oriented data formats in the big data processing space. It leverages various techniques to store data in a CPU- and I\/O-efficient way. Furthermore, it has the capabilities to push-down analytical queries on the data to the I\/O layer to avoid the loading of nonrelevant data chunks. With various Java and a C++ implementation, Parquet is also the perfect choice to exchange data between different technology stacks.\n\nAs part of this talk, a general introduction to the format and its techniques will be given. Their benefits and some of the inner workings will be explained to give a better understanding how Parquet achieves its performance. At the end, benchmarks comparing the new C++ & Python implementation with other formats will be shown.",
          "onhold": false,
          "bio": "Uwe Korn is a Data Scientist at the German RetailTec company Blue Yonder. His expertise is on building architectures for machine learning services that are scalably usable for multiple customers aiming at high service availability as well as rapid prototyping of solutions to evaluate the feasibility of his design decisions. As part of his work to provide an efficient data interchange he became a core committer to the Apache Parquet project.",
          "request_id": "4a3fd028837301ef585a45d",
          "category": "Column",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.9524,
          "notes": "",
          "tags": "",
          "id": "4a3fd028837301ef585a45d",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Parquet Format in Practice & Detail",
          "speaker": "Uwe L. Korn",
          "accepted": true,
          "submitter": "037947fa1dd2f78abb4c6e02047a34d1cf17cc55a4dec7b15b7a0201"
        },
        "room": "90ab76991543bd7a2",
        "day": 1479340800,
        "assignee": "4a3fd028837301ef585a45d",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479390600
      }]
    }, {
      "day": 1479254400,
      "slots": [{
        "starttime": 1479294000,
        "origin": "90ab76991543bd7a2",
        "id": "a62fdd2ca982c4479",
        "duration": 50,
        "readonly": false,
        "request_id": "a62fdd2ca982c4479",
        "talk": {
          "description": "During the lasts three years we use kappa architecture in almost all our projects. We want to show how kappa architecture fixed in very different size projects. Kappa architecture is not the silver bullets for every project but is very likely ...",
          "onhold": false,
          "bio": "President Hispalinux (Spanish User Local Group) (1999-2007) Author of the book \"La Pastilla Roja\" the first book in spanish about free software (2004) More than 200 lectures around the world. Now CDO of Open Sistemas and advocate of Apache Spark and Kappa Architecture. Organize of Machine Learning Spain Meetup.",
          "request_id": "3e565b71f6a3ca4d3c1dbb4",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.5011,
          "notes": "",
          "tags": "",
          "id": "3e565b71f6a3ca4d3c1dbb4",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Real use cases of Kappa Architecture",
          "speaker": "Juantomas Garcia",
          "accepted": true,
          "submitter": "95b0df92cde52793816a1cd254e00a82cff1128c8cdb074cc87825c8"
        },
        "room": "90ab76991543bd7a2",
        "day": 1479254400,
        "assignee": "3e565b71f6a3ca4d3c1dbb4",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479297000
      }, {
        "starttime": 1479297600,
        "origin": "90ab76991543bd7a2",
        "id": "6235b4344af141520",
        "duration": 50,
        "readonly": false,
        "request_id": "6235b4344af141520",
        "talk": {
          "description": "Spark is being deployed in production by many enterprises. With enterprise traction comes enterprise security requirements and the need to meet enterprise security standards. \n\n\n\nThe sessions walks through enterprise security requirements, provides deep of dive Spark security features and shows how Spark meets these enterprise security requirements. \n\n\n\nThe talks go on uncovering the entire gamut of security in Spark from Kerberos, Authentication, Authorization, Audit to Encryption with Spark. The session will provide deep dive on all existing security features in Spark and will also outline to future security work planned in the Apache Spark community.",
          "onhold": false,
          "bio": "Vinay Shukla is the Director of Product Management for Spark & Zeppelin at Hortonworks. Previously, Vinay has worked as Developer and Security Architect.  Vinay has given talks at Hadoop Summit (2x), Apache Con Big Data - Europe (2015), JavaOne & Oracleworld. His most recent talk was at Hadoop Summit Dublin (April, 2016) -Running Spark in Production. Please see video of that talk at (https:\/\/youtu.be\/OkyRdKahMpk) and slides at (http:\/\/www.slideshare.net\/vinnies12\/running-spark-in-production)\n\n",
          "request_id": "c74e57e1d8a1f3aad5d4d96",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.4869,
          "notes": "",
          "tags": "",
          "id": "c74e57e1d8a1f3aad5d4d96",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Apache Spark: Enterprise Security for production deployments",
          "speaker": "Vinay Shukla",
          "accepted": true,
          "submitter": "634b78056580858c106868e7cc4682962ad1260433c2f38acc8875d9"
        },
        "room": "90ab76991543bd7a2",
        "day": 1479254400,
        "assignee": "c74e57e1d8a1f3aad5d4d96",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479300600
      }, {
        "starttime": 1479301200,
        "origin": "90ab76991543bd7a2",
        "id": "3003ea130f07e75d4",
        "duration": 50,
        "readonly": false,
        "request_id": "3003ea130f07e75d4",
        "talk": {
          "description": "Apache Geode (incubating) is a distributed in-memory data grid built for high throughput low latency applications. Data stored in a Gode cluster can be accessed by Geode clients (which talk to the server over TCP) and over REST api. One can also manage the Geode cluster over JMX and rest api. \n\nAlthough you could secure the transport using ssl, role based access control existed only for clients over TCP. In the latest release of Apache Geode, we now have role based access control for all Geode APIs, and we used Apache Shiro for our implementation. In this talk we will provide details on how this was accomplished and present our äóìlessons learnedäó.\n\n",
          "onhold": false,
          "bio": "I am a Software Engineer at Pivotal and a committer on Apache Geode, have been working on the project for the past 8 years, even before it became a part of ASF.\n\nPrevious presentations:\n\nSpring One (2010)\n\nGeode Clubhouse (2015)\n\nJava User group meetup Seattle (Feb 2016)",
          "request_id": "f44303be35151afb1875364",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.5379,
          "notes": "",
          "tags": "",
          "id": "f44303be35151afb1875364",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Implementing security in Apache Geode using Apache Shiro",
          "speaker": "Swapnil Bawaskar",
          "accepted": true,
          "submitter": "f505cf1cc97d229926178c7ebfbdf3620d29426b087747ba6601d26e"
        },
        "room": "90ab76991543bd7a2",
        "day": 1479254400,
        "assignee": "f44303be35151afb1875364",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479304200
      }, {
        "starttime": 1479310200,
        "origin": "90ab76991543bd7a2",
        "id": "cab163ef35e174b5d",
        "duration": 50,
        "readonly": false,
        "request_id": "cab163ef35e174b5d",
        "talk": {
          "description": "Over the past few years, there was a tremendous demand for security, especially access control, from Hive users. Traditionally, access control at the row and column level in Hive is implemented through views. However, this view-based method turns out to be not only ineffective when view definitions become too complex but also costly when a large number of views must be manually updated and maintained. In this talk, we will talk about pluggable and extensible access control which helps resolve all these problems. Based on a security policy that specifies the rules and conditions under which a user, group, or role can access rows, columns, or both of a base table, the new access control is activated on the underlying base table. We will not only show a live demo of how a pluggable and extensible security policy works with Apache Hive but also deep dive into the implementation.",
          "onhold": false,
          "bio": "Pengcheng Xiong is a staff software engineer at Hortonworks. He is now working on the next-generation Hive optimizer and he serves and contributes as an Apache Hive PMC member and committer. He has extensive research and development experiences in centralized\/distributed RDBMS internals, Hive and etc. He has dozens of publications in database conferences, e.g., SIGMOD, VLDB and ICDE. He holds a PhD from Georgia Tech.",
          "request_id": "7d0c291b730d9952cf8ef68",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.7615,
          "notes": "",
          "tags": "",
          "id": "7d0c291b730d9952cf8ef68",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Pluggable and extensible access control to a table\/view",
          "speaker": "pengcheng xiong",
          "accepted": true,
          "submitter": "f4913b5da0570e38e9ad1e77030b12112ff3e02be86ce3536119e224"
        },
        "room": "90ab76991543bd7a2",
        "day": 1479254400,
        "assignee": "7d0c291b730d9952cf8ef68",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479313200
      }, {
        "starttime": 1479313800,
        "origin": "90ab76991543bd7a2",
        "id": "ef1be7c5c7da5870b",
        "duration": 50,
        "readonly": false,
        "request_id": "ef1be7c5c7da5870b",
        "talk": {
          "description": "As big data continues to get bigger, deploying flexible and robust security is more important than ever.  In this talk, we'll discuss about Apache Sentry, a central service for policy management and its various pluggable authorization engines which integrates with many Hadoop components. And we will dive deep into how its latest design allows for fault tolerance, high availability and scalability.\n\n\n\nUnlike traditional database systems, authorization in Hadoop eco system is a tricky problem due to the fact that there are multiple doors to the same data. Sentry provides a great deal of usability by letting users define policies once and it replicates the state as necessary. With this comes additional challenges of designing a distributed service which manages consistent state. This talk will touch upon core design choices which lie as building blocks to any robust distributed system.",
          "onhold": false,
          "bio": "Sravya Tirukkovalur is a software engineer at Cloudera working on Hadoop security. She is one of the active contributors to the Apache Sentry project and also the PMC Chair. She got her Masters degree from The Ohio State University, with her research focus on High performance and Distributed computing. She is passionate about social impact through technology and volunteers outside of her day job. See https:\/\/www.linkedin.com\/in\/sravya-tirukkovalur\n\n\\n\n\nHao Hao is a software engineer at Cloudera. She is an active contributor and a PMC member of Apache Sentry project. Hao has performed extensive research on smartphone security, web security while she was a PhD student at Syracuse University. Prior to joining Cloudera, Hao worked at eBayäó»s Search Backend team to build search infrastructure for eBayäó»s online buying platform. ",
          "request_id": "e048ba8eab394de7522ecaa",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.0932,
          "notes": "",
          "tags": "",
          "id": "e048ba8eab394de7522ecaa",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Apache Sentry - High availability",
          "speaker": "Sravya Tirukkovalur",
          "accepted": true,
          "submitter": "7105ab19c0a12d521b9fc87b4f41c6a72926090626b19406a75e47f6"
        },
        "room": "90ab76991543bd7a2",
        "day": 1479254400,
        "assignee": "e048ba8eab394de7522ecaa",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479316800
      }]
    }, {
      "day": 1479168000,
      "slots": [{
        "starttime": 1479207600,
        "origin": "90ab76991543bd7a2",
        "id": "78e57e5a352d5afb4",
        "duration": 50,
        "readonly": false,
        "request_id": "78e57e5a352d5afb4",
        "talk": {
          "description": "Stream processing goes mainstream in the Big Data world and becomes widely adopted in the industry. Despite its expanding popularity, many hard problems remain to be solved. Apache Gearpump(incubating) is a next-gen streaming engine designed to solve the hard parts in stream processing. It is good at streaming infinite out-of-order data and guarantees correctness. It helps user to easily program streaming applications, get runtime information and update dynamically. In this presentation, Manu Zhang will demystify how Gearpump solves the hard parts in stream processing and achieves high throughput at millisecond latency message delivery.",
          "onhold": false,
          "bio": "I'm currently a Big Data engineer at Intel Shanghai focused on stream processing. I'm also an active committer of Apache Gearpump(incubating) and contributor of Apache Beam(incubating) and Apache Storm. Previously I have given talk at Strata + Hadoop World Beijing, 2016 and spoken at Shanghai Big Data Streaming Meetup. ",
          "request_id": "1eed645c3d7171c70e43b95",
          "category": "Stream",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.7973,
          "notes": "",
          "tags": "",
          "id": "1eed645c3d7171c70e43b95",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Apache Gearpump next-gen streaming engine",
          "speaker": "Manu Zhang",
          "accepted": true,
          "submitter": "10590a84063cd3ae8a4796ad69ceee16a4c8f9116a965dbbaddcea92"
        },
        "room": "90ab76991543bd7a2",
        "day": 1479168000,
        "assignee": "1eed645c3d7171c70e43b95",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479210600
      }, {
        "starttime": 1479211200,
        "origin": "90ab76991543bd7a2",
        "id": "a48a66df51ae40c9e",
        "duration": 50,
        "readonly": false,
        "request_id": "a48a66df51ae40c9e",
        "talk": {
          "description": "Spark Streaming is currently one of the leading frameworks in the industry for distributed stream processing. However testing Spark Streaming programs is still a challenge, due to the complications of dealing with time. In this presentation, Adrian Riesco gives an introduction to sscheck, a testing library for Spark that extends ScalaCheck with additional temporal logic operators for generators and properties, that are used to define tests for Spark Streaming as linear temporal logic formulas, resulting in tests that are high level and easy to understand.",
          "onhold": false,
          "bio": "\n\nI currently work at Universidad Complutense de Madrid, Spain. My research interests include formal methods, logic,\n\ndebugging, and testing. I have given more than 20 talks at international conferences in computer science. More details are available at http:\/\/maude.sip.ucm.es\/~adrian\/. The work here is a joint work with Juan Rodrí_guez-Hortalíç (http:\/\/dblp.uni-trier.de\/pers\/hd\/r\/Rodr=iacute=guez=Hortal=aacute=:Juan.html), who moved to the SW development industry and specialized in Hadoop related technologies in 2012, after 6 years in the academia. We present here our\n\njoint project sscheck, a property-based testing library for Spark Streaming https:\/\/github.com\/juanrh\/sscheck\/wiki\/Quickstart.",
          "request_id": "c2f821a0d945cb48488a70c",
          "category": "stream",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.2724,
          "notes": "",
          "tags": "",
          "id": "c2f821a0d945cb48488a70c",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Property-based Testing for Spark Streaming",
          "speaker": "Adrian Riesco",
          "accepted": true,
          "submitter": "dd536ad313dfc2d9cf4f407635f5e2814ff1224ec3aa03a551ae260f"
        },
        "room": "90ab76991543bd7a2",
        "day": 1479168000,
        "assignee": "c2f821a0d945cb48488a70c",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479214200
      }, {
        "starttime": 1479214800,
        "origin": "90ab76991543bd7a2",
        "id": "dbc6a93508adf3a9b",
        "duration": 50,
        "readonly": false,
        "request_id": "dbc6a93508adf3a9b",
        "talk": {
          "description": "Stream processing applications built on Apache Apex run on Hadoop clusters and typically power analytics use cases where availability, flexible scaling, high throughput, low latency and correctness are essential. These applications consume data from a variety of sources, including streaming sources like Apache Kafka, Kinesis or JMS, file based sources or databases. Processing results often need to be stored in external systems (sinks) for downstream consumers (pub-sub messaging, real-time visualization, Hive and other SQL databases etc.). Apex has the Malhar library with a wide range of connectors and other operators that are readily available to build applications. We will cover key characteristics like partitioning and processing guarantees, generic building blocks for new operators (write-ahead-log, incremental state saving, windowing etc.) and APIs for application specification.",
          "onhold": false,
          "bio": "Thomas is Apache Apex PMC member and architect\/co-founder at DataTorrent. He has developed distributed systems, middleware and web applications since 1997. Prior to DataTorrent he was in the Hadoop Team at Yahoo! and contributed to projects like Pig and Hive and migration to next generation Hadoop 2.x.",
          "request_id": "a298345e81c9f90ef47529c",
          "category": "Stream",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.9681,
          "notes": "",
          "tags": "",
          "id": "a298345e81c9f90ef47529c",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Building Streaming Applications with Apache Apex",
          "speaker": "Thomas Weise",
          "accepted": true,
          "submitter": "10fc0d8b1ea7e46f556f1e253fa7d90827a3538cc8918b729eae3f73"
        },
        "room": "90ab76991543bd7a2",
        "day": 1479168000,
        "assignee": "a298345e81c9f90ef47529c",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479217800
      }, {
        "starttime": 1479223800,
        "origin": "90ab76991543bd7a2",
        "id": "534560ad0245aa660",
        "duration": 50,
        "readonly": false,
        "request_id": "534560ad0245aa660",
        "talk": {
          "description": "Streaming Report (Mao Wei, Intel) - Streaming processing technology developed so fast recently. Spark Streaming, Flink, Storm, Heron, Gearpump, so many choices are available when people want to pick up the proper one to resolve their real business problems. In this presentation, Mao Wei will go thought all of these different frameworks and compare them in detail. From functional aspect, Wei will discuss underlying mechanism of these frameworks and review several function points which users may care about generally. And from practical aspect, you will see a performance test result based on HiBench, which is a cross platforms micro benchmark suite for big data open sourced by Intel BDT. The test cases include identity, repartition, state operation and window operation.",
          "onhold": false,
          "bio": "I am working in Intel Big Data Team, our team has contributed to Spark since 2012, and focus on Flink and other open source big data platforms at the same time. And we did lots optimization in big data area based on Intel Architecture. For myself, I am dedicated to streaming processing technology.",
          "request_id": "c781890c64c1c2602201081",
          "category": "Stream",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.8794,
          "notes": "",
          "tags": "",
          "id": "c781890c64c1c2602201081",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Streaming Report: Functional Comparison and Performance Evaluation",
          "speaker": "Wei Mao",
          "accepted": true,
          "submitter": "3ec2a2d0afe0bd90189069b1d1cf23f7937310a843851b7463a1de60"
        },
        "room": "90ab76991543bd7a2",
        "day": 1479168000,
        "assignee": "c781890c64c1c2602201081",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479226800
      }, {
        "starttime": 1479227400,
        "origin": "90ab76991543bd7a2",
        "id": "75239804f48e21cd0",
        "duration": 50,
        "readonly": false,
        "request_id": "75239804f48e21cd0",
        "talk": {
          "description": "The talk shall focus on how to develop applications in real time analytics space using Apache Calcite's advanced query planning capabilities. The talk shall give a small overview of Calcite's planner and rules engine and then proceed to discuss the capabilities that can be used to develop real time applications that continuously stream data and process them. The talk shall be discussing the ongoing work in Calcite's framework and the upcoming streaming aggregation features that will be present soon. The talk shall also focus on Calcite's highly adaptable framework that allows Calcite to work with many existing projects and how your current application can take advantage of Calcite' s planning and aggregation capabilities.",
          "onhold": false,
          "bio": "An Apache Apex committer where he is engaged in designing and implementing next generation features and performing reviews.A learning PostgreSQL hacker who is currently engaged in various aspects of Postgres.He has been an active contributor,implementing ordered set functions, implementing grouping sets in Postgresql, improving sort and hashjoin performance and OLAP performance. He is also a committer for Apache HAWQ, Apache MADLib and has been involved in development of Greenplum database, having implemented next generation Resource Management and having been responsible for addition of various improvements in Greenplum's query planner. He is hacking Apache Calcite in which he is hacking streaming aggregates.",
          "request_id": "cb84be2a402a3cf56a15110",
          "category": "Stream",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.2501,
          "notes": "",
          "tags": "",
          "id": "cb84be2a402a3cf56a15110",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Real Time Aggregates in Apache Calcite -- Optimal Use of your Streaming Data",
          "speaker": "Atri Sharma",
          "accepted": true,
          "submitter": "3e7b55f73ea759f9cbfccae833b6884a5cea16d7a3f4056aeb093271"
        },
        "room": "90ab76991543bd7a2",
        "day": 1479168000,
        "assignee": "cb84be2a402a3cf56a15110",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479230400
      }]
    }]
  }, {
    "id": "91b3ec58a632b5d03",
    "name": "Giralda I\/II",
    "days": [{
      "day": 1479340800,
      "slots": [{
        "starttime": 1479380400,
        "origin": "91b3ec58a632b5d03",
        "id": "e447006948afa8177",
        "duration": 50,
        "readonly": false,
        "request_id": "e447006948afa8177",
        "talk": {
          "description": "Developers are a possible attack vector for targeted attacks to infiltrate malicious code \n\ninto enterprises.\n\n\n\nThe Speaker did a network traffic analysis with the Bro Network Security Monitor (bro.org)\n\nbacked by an ELK Stack while compiling Apache Bigtop, a Big Data Distribution containing\n\nApache Hadoop, Spark, HBase, Hive, Flink et al.\n\n\n\nWhile there are no obvious traces of a malicious code within the traffic, there are many\n\nfindings of possible attack vectors like unsecurely configured critical software infrastructure \n\nservers, usage of private repositories or unsecure protocols.\n\n\n\nThe Analysis showed that many compile jobs are downloading and running executables from untrusted sources. \n\nThe author will shortly explain how these weaknesses can be exploited and will give recommendations on how to resolve these issues.",
          "onhold": false,
          "bio": "Dr. Olaf Flebbe received his PhD in computational physics in Tí_bingen, Germany. He works as the chief software architect at science+computing ag. He is a member of the PMC of Apache Bigtop. Occasionally he gives talks about random projects at various conferences.",
          "request_id": "f63033661f0fe989b4293f0",
          "category": "Security",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.0208,
          "notes": "",
          "tags": "",
          "id": "f63033661f0fe989b4293f0",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Attacking a Big Data Developer",
          "speaker": "Olaf Flebbe",
          "accepted": true,
          "submitter": "ecd5f223458f7565588c2300db06c667ef3f8c729ed8e472cc6c98e0"
        },
        "room": "91b3ec58a632b5d03",
        "day": 1479340800,
        "assignee": "f63033661f0fe989b4293f0",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479383400
      }, {
        "starttime": 1479384000,
        "origin": "91b3ec58a632b5d03",
        "id": "0814e6bbeff3d125a",
        "duration": 50,
        "readonly": false,
        "request_id": "0814e6bbeff3d125a",
        "talk": {
          "description": "Big Data forensics is a new type of forensics, just as Big Data is a new way of solving the challenges presented by large, complex data. Thanks to the growth in data and the increased value of storing more data and analyzing it fasteräóîBig Data solutions have become more common and more prominently positioned within organizations. As such, the value of Big Data systems has grown, often storing data used to drive organizational strategy, identify sales, and many different modes of electronic communication. The forensic value of such data is obvious: if the data is useful to an organization, then the data is valuable to an investigation of that organization. The information in a Big Data system is not only inherently valuable, but the data is most likely organized and analyzed in such a way to identify how the organization treated the data.\n\nBig Data forensics is the forensic collection and",
          "onhold": false,
          "bio": "Deepak S. Mane is a Performance\/Cloud Consultant in Global Consulting practice - Performance Engineering Lab at Tata Research Development and Design Center (A Research wing of TCS). He has published 14 papers in Conference Seminars , and   He is currently pursuing research in Cloud computing, Performance Management,",
          "request_id": "f3a3dbd5220d5cf41f4b2e7",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.4932,
          "notes": "",
          "tags": "",
          "id": "f3a3dbd5220d5cf41f4b2e7",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Big Data Forensic analysis using Linux ",
          "speaker": "Deepak Mane",
          "accepted": true,
          "submitter": "a8fe445ef11f9a2a46d593895c5634ba96d1c5ce839f2a14876cf5f0"
        },
        "room": "91b3ec58a632b5d03",
        "day": 1479340800,
        "assignee": "f3a3dbd5220d5cf41f4b2e7",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479387000
      }, {
        "starttime": 1479387600,
        "origin": "91b3ec58a632b5d03",
        "id": "0e17bac0bd38aa501",
        "duration": 50,
        "readonly": false,
        "request_id": "0e17bac0bd38aa501",
        "talk": {
          "description": "Reliable, high-rate ingestion of data from a large variety of sources is the first step towards answering big analytical questions, and Apache Kafka, a scalable publish-subscribe messaging system, is a popular choice for this goal. With the increasing adoption of Kafka, security has become more important than ever. In this talk, Ashish Singh will review recent advancements made in Kafka towards closing security gaps, and discuss how addition of pluggable authorization in Kafka has enabled Apache Sentry to provide an enterprise-grade, fine-grained, role-based authorization in Kafka. The talk will conclude with a demonstration of a working example of how administrators can rely on Sentry for enforcing fine-grained authorization in multi-tenant Kafka platforms.",
          "onhold": false,
          "bio": "Ashish Singh is a Software Engineer, working with Cloudera to empower the Hadoop ecosystem to answer bigger questions. Ashish studied Computer Science and Engineering at Ohio State University. Before working in the Big Data space, he worked on optimizing MPI collective communications on High Performance Computing clusters. As part of the ingest team at Cloudera, he is interested in making data movement easier in large-scale data architectures. Ashish is a committer on Apache Sentry and actively contributes to Apache Kafka.",
          "request_id": "1af47fb0a064f4233a3403c",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.5068,
          "notes": "",
          "tags": "",
          "id": "1af47fb0a064f4233a3403c",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Fine grained authorization in Apache Kafka",
          "speaker": "Ashish Singh",
          "accepted": true,
          "submitter": "cf10020cad486462c0dde3f377ebe959776b60d2846cbdbf725d3a2c"
        },
        "room": "91b3ec58a632b5d03",
        "day": 1479340800,
        "assignee": "1af47fb0a064f4233a3403c",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479390600
      }]
    }, {
      "day": 1479168000,
      "slots": [{
        "starttime": 1479207600,
        "origin": "91b3ec58a632b5d03",
        "id": "f382357fee1775171",
        "duration": 50,
        "readonly": false,
        "request_id": "f382357fee1775171",
        "talk": {
          "description": "Web app developers want to take advantage of the sophisticated analytics and big data processing provided by engines such as Apache Spark. Traditionally, web app development would use an enterprise language like Java but with a development emphasis now on agility and simplicity, technologies such as Node.js, Ruby on Rails, and PHP are increasingly being used for such development. Apache Spark has APIs for Scala, Java and Python but no API for Node.js or JavaScript despite their importance for web app development. To fill this gap, the EclairJS open-source project was created to provide an API in Node.js and JavaScript and enable web app developers to incorporate the analytic and other capabilities of Spark. In this presentation, David Fallside will show some web applications that demonstrate Sparkäó»s capabilities and explain how they are implemented using EclairJS.",
          "onhold": false,
          "bio": "David Fallside works in an emerging tech team at IBM that develops the open-source EclairJS project and provides Node.js with an Apache Spark API. Some of the teamäó»s previous projects include LoB tools for Spark and Hadoop, and information engineering for IBMäó»s Watson. David was responsible for creating the Apache Derby project, and has worked in several W3C working groups including XML Schema, XML SOAP, and the W3C Advisory Board. He has a PhD in Cognitive Psychology from Carnegie Mellon University. David's recent public speaking experience includes \"EclairJS = Node.js + Apache Spark\" at the Spark Summit, San Francisco 2016, \"Machine Learning Outlook\" at InterConnect, Las Vegas 2016 (Inner Circle Presentation); co-presenter on R.Smith's \"Road to Real-Time Digital Business\" at Strata+Hadoop, London 2015; press interview (https:\/\/youtu.be\/3OMOax7dDPE) at Strata+Hadoop, Barcelona 2014.",
          "request_id": "95cd691ebc0948e402fbca6",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.4243,
          "notes": "",
          "tags": "",
          "id": "95cd691ebc0948e402fbca6",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Putting A Spark in Web Apps",
          "speaker": "David Fallside",
          "accepted": true,
          "submitter": "548d647fd4a1d4b92fe4379a2820d224e84b6d636c506856d3e9d32b"
        },
        "room": "91b3ec58a632b5d03",
        "day": 1479168000,
        "assignee": "95cd691ebc0948e402fbca6",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479210600
      }, {
        "starttime": 1479211200,
        "origin": "91b3ec58a632b5d03",
        "id": "5685fd1960ed2afc4",
        "duration": 50,
        "readonly": false,
        "request_id": "5685fd1960ed2afc4",
        "talk": {
          "description": "There are many resources available for using Apache Spark to build collaborative filtering models. However, there are relatively few for how to build a large-scale, end-to-end recommender system.\n\n\n\nThis talk will show how to create such a system, using Apache Kafka, Spark Streaming and Elasticsearch for data ingestion, real-time analytics and data storage, Spark DataFrames and ML pipelines for data aggregation and model building, and Elasticsearch for model management, serving and data visualization. We will also explore techniques for scaling model serving, using Spark Streaming for real-time model updates, and how to incorporate state-of-the-art models into this framework.\n\n\n\nThe talk will be technical and developer-focused, highlighting experiences from building real-world recommender systems, and providing example code (which will be available as open source).",
          "onhold": false,
          "bio": "Nick is a Principal Engineer at IBM, working primarily on machine learning on Apache Spark. He is a member of the Apache Spark PMC and author of Machine Learning with Spark. Previously, he co-founded Graphflow, a machine learning startup focused on recommendations. He has also worked at Goldman Sachs, Cognitive Match and Mxit. He is passionate about combining commercial focus with machine learning and cutting-edge technology to build intelligent systems that learn from data to add business value.",
          "request_id": "79427e7d8238078b3db68b4",
          "category": "Spark",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.8747,
          "notes": "",
          "tags": "",
          "id": "79427e7d8238078b3db68b4",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Building a Scalable Recommendation Engine with Apache Spark, Apache Kafka and Elasticsearch",
          "speaker": "Nick Pentreath",
          "accepted": true,
          "submitter": "13c1712537d537523bef0aa701a91b872cd92bdd08aa11e704ef7143"
        },
        "room": "91b3ec58a632b5d03",
        "day": 1479168000,
        "assignee": "79427e7d8238078b3db68b4",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479214200
      }, {
        "starttime": 1479214800,
        "origin": "91b3ec58a632b5d03",
        "id": "6daac0efa149deeaa",
        "duration": 50,
        "readonly": false,
        "request_id": "6daac0efa149deeaa",
        "talk": {
          "description": "Apache Spark based applications are often comprised of many separate, interconnected components that are a good match for an orchestrated containerized platform like Kubernetes. But with the increased flexibility afforded by these technologies comes a new set challenges for building rich data-centric applications.\n\n\n\nIn this presentation we will discuss techniques for building multi-component Apache Spark based applications that can be easily deployed and managed on a Kubernetes infrastructure. Building on experiences learned while developing and deploying cloud native applications on an OpenShift platform, we will explore common issues that arise during the engineering process and demonstrate workflows for easing the maintenance factors associated with complex installations.",
          "onhold": false,
          "bio": "Michael is a software developer in Red Hat's emerging technology group. He is a contributor to, and core reviewer for the Sahara project, the OpenStack Security Project, and the OpenStack API Working Group. For the past year he has been working on creating and deploying Apache Spark streaming applications on the OpenStack and OpenShift platforms.",
          "request_id": "66d63ef8eec810a465dd8eb",
          "category": "Spark",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.9116,
          "notes": "",
          "tags": "",
          "id": "66d63ef8eec810a465dd8eb",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Building Apache Spark application pipelines for the Kubernetes Ecosystem",
          "speaker": "Michael McCune",
          "accepted": true,
          "submitter": "1db6bebccda156129ace109052b0ff62f004980d4486948636349499"
        },
        "room": "91b3ec58a632b5d03",
        "day": 1479168000,
        "assignee": "66d63ef8eec810a465dd8eb",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479217800
      }, {
        "starttime": 1479223800,
        "origin": "91b3ec58a632b5d03",
        "id": "7be852bc11d23d891",
        "duration": 50,
        "readonly": false,
        "request_id": "7be852bc11d23d891",
        "talk": {
          "description": "As software becomes more free and open it also is becoming more complex and expensive to operate. How can we as an Open Source community distill best practices, and recommended operations to model complex interconnected services so users can focus on their ideas? How can we as developers deliver recommended best practices in our applications and when connected to other applications so users are free to contribute and use the project on their choice of substrate (laptop, cloud, or bare metal [x86, ARM, ppc64el, s390x]). \n\nIn this talk we explore how Juju can provide an Open Source method to model a multi-node Apache Spark cluster across a diverse set of substrates, and start adding other services to build additional solutions. This talk will include a demo, and users should be able to take all software shown to try for themselves in a free and Open Source manner.",
          "onhold": false,
          "bio": "",
          "request_id": "225be40ba0e580eb1c4b574",
          "category": "spark",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.5656,
          "notes": "",
          "tags": "",
          "id": "225be40ba0e580eb1c4b574",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Open Source Operations: Building on Apache Spark with InsightEdge, TensorFlow, Apache Zeppelin, and\/or Apache <your-project>",
          "speaker": "Antonio Rosales",
          "accepted": true,
          "submitter": "22f34548f15fa04d3dbc5b7fbbfa527367c12a0a1fa5d2b4d45554e9"
        },
        "room": "91b3ec58a632b5d03",
        "day": 1479168000,
        "assignee": "225be40ba0e580eb1c4b574",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479226800
      }, {
        "starttime": 1479227400,
        "origin": "91b3ec58a632b5d03",
        "id": "188f9a307502454fe",
        "duration": 50,
        "readonly": false,
        "request_id": "188f9a307502454fe",
        "talk": {
          "description": "R is a very popular platform for Data Science. Apache Spark is a highly scalable data platform. How could we have the best of both worlds? In this talk we will walkthrough many examples how several new features in Apache Spark 2.0.0 will enable this. We will also look at exciting changes coming next in Apache Spark 2.0.1 and 2.1.0.\n\n\n\n",
          "onhold": false,
          "bio": "Felix Cheung is a Committer of Apache Spark and a PMC\/Committer of Apache Zeppelin. He has been active in the Big Data space for 3+ years, he is a co-organizer of the Seattle Spark Meetup, presented several times and he was a teaching assistant to the very popular edx Introduction to Big Data with Apache Spark, and Scalable Machine Learning MOOCs in the summer of 2015. He had a tutorial session at ApacheCon: Big Data North America 2016\n\n",
          "request_id": "5be2cbcceffe065b60a22b5",
          "category": "Spark",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.8085,
          "notes": "",
          "tags": "",
          "id": "5be2cbcceffe065b60a22b5",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Scalable Data Science in R and Apache Spark 2.0",
          "speaker": "Felix Cheung",
          "accepted": true,
          "submitter": "a2841447b55094a119e0228e899135682fa5abaf3b94eec2a19a12f3"
        },
        "room": "91b3ec58a632b5d03",
        "day": 1479168000,
        "assignee": "5be2cbcceffe065b60a22b5",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479230400
      }]
    }, {
      "day": 1479254400,
      "slots": [{
        "starttime": 1479294000,
        "origin": "91b3ec58a632b5d03",
        "id": "468e7d2ca1a3eed9c",
        "duration": 50,
        "readonly": false,
        "request_id": "468e7d2ca1a3eed9c",
        "talk": {
          "description": "Apache Spark has rocked the big data landscape, becoming the largest open source big data community with over 750 contributors from more than 200 organizations. Spark's core tenants of speed, ease of use, and its unified programming model fit neatly with the high performance, scalable, and manageable characteristics of modern Java runtimes. In this talk Tim Ellison, a JVM developer at IBM, shows some of the unique Java 8 capabilities in the JIT compiler, fast networking, serialization techniques, and GPU off-loading that deliver the ultimate big data platform for solving business problems. Tim will demonstrate how solutions, previously infeasible with regular Java programming, become possible with this high performance Spark core runtime, enabling you to solve problems smarter and faster.",
          "onhold": false,
          "bio": "Tim Ellison is currently a Senior Technical Staff Member with IBM's Java Technology Centre in the UK. He has worldwide responsibility for Open Source Engineering in the Java SDK underpinning a broad selection of IBM's flagship products. He is a Member of the Apache Software Foundation, a PMC member of Apache Pirk, and has been a Vice President of the Apache Software Foundation and chair of the Apache Harmony Project. Tim is an emeritus team leader and committer at the Eclipse Foundation. Tim holds a BSc in Computer Science, and an MSc in Computer System Design from the University of Manchester. Tim has contributed to the commercial implementation of Smalltalk, IBM VisualAge Micro Edition, Eclipse, and the Java SDK over a period of over twenty years; and spoken internationally at many conferences such as ApacheCon, EclipseCon, JavaOne, jFokus, JAX, JavaZone, and more.",
          "request_id": "c87994a05cdbc592d0043c6",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.2407,
          "notes": "",
          "tags": "",
          "id": "c87994a05cdbc592d0043c6",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "A Java Implementer's Guide to Boosting Apache Spark Performance",
          "speaker": "Tim Ellison",
          "accepted": true,
          "submitter": "cc85c1c39a78880eb792aad3c0c2c8170f9a18e03e8b08b23abdc04b"
        },
        "room": "91b3ec58a632b5d03",
        "day": 1479254400,
        "assignee": "c87994a05cdbc592d0043c6",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479297000
      }, {
        "starttime": 1479297600,
        "origin": "91b3ec58a632b5d03",
        "id": "a61303ed2f85dd4f2",
        "duration": 50,
        "readonly": false,
        "request_id": "a61303ed2f85dd4f2",
        "talk": {
          "description": "Making historical data available for searching can be a challenge, especially if you have a lot of it. Indexing data to a live cluster can degrade search performance and having a spare cluster where you index your data can be expensive. In this talk we present the approaches we tried and describe an approach to create ElasticSearch indices offline using Apache Spark. When created, these indices are then stored as snapshots in HDFS and can then be restored to a running ElasticSearch cluster. Snapshots in HDFS also work as a backup, ready to restore solution in case of an error.",
          "onhold": false,
          "bio": "Software Engineer at ESET\n\n\n\nCurrently working with Big Data technologies at ESET. Responsible for collecting and storing and making data available for end users.\n\n\n\nPreviously worked at Honeywell.\n\n\n\nSpeaking experience:\n\nCaro workshop 2016 (http:\/\/2016.caro.org\/)",
          "request_id": "0a4304465b9c5087aab6d39",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.1714,
          "notes": "",
          "tags": "",
          "id": "0a4304465b9c5087aab6d39",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Using Apache Spark for generating ElasticSearch indices offline",
          "speaker": "Andrej Babolcai",
          "accepted": true,
          "submitter": "341389ad20837bda1e81ca5862e40a1d9013d7a45721f66fc0f98670"
        },
        "room": "91b3ec58a632b5d03",
        "day": 1479254400,
        "assignee": "0a4304465b9c5087aab6d39",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479300600
      }, {
        "starttime": 1479301200,
        "origin": "91b3ec58a632b5d03",
        "id": "66ff56cb93c2fa4d7",
        "duration": 50,
        "readonly": false,
        "request_id": "66ff56cb93c2fa4d7",
        "talk": {
          "description": "Apache Spark is the hottest computing engine nowadays.\n\nMore people in big data industry start to use Spark in production systems, \n\nsuch as machine learning and data ETL applications. \n\nHowever, developers take quality of code seriously in production systems.\n\nIn the past experience, we find there is a gap between development and production.\n\nThe difficulties we meet when developing Spark applications in production systems are:\n\n(1) hard to communicate with components,\n\n(2) indirect management of application arguments,\n\n(3) inadequate code maintainability.\n\nTo solve these problems and make development smoother, we propose\n\na dependency-injection-based programming framework on JVM systems.\n\nIt provides basic management, monitoring, and better communication mechanisms.\n\nThe huge flexibility can help developers writing Spark applications and integrating with components in a better manner.",
          "onhold": false,
          "bio": "Chun-Ting Kuo (Bruce) works at Yahoo as a data engineer, and he dedicates his work on developing data products and   scientific applications. His experience covers Spark, Hadoop, algorithms, and a little machine learrning. When he is free, he loves to code and know novel techniques.",
          "request_id": "902c9f46161b51de5d07d91",
          "category": "Spark",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.1105,
          "notes": "",
          "tags": "",
          "id": "902c9f46161b51de5d07d91",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Elastic Spark Programming Framework - A Dependency Injection Based Programming Framework for Spark Applications",
          "speaker": "Bruce Kuo",
          "accepted": true,
          "submitter": "29806c7d4b3d188fd0068683240c061b60c36dd886eabb46104bb792"
        },
        "room": "91b3ec58a632b5d03",
        "day": 1479254400,
        "assignee": "902c9f46161b51de5d07d91",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479304200
      }, {
        "starttime": 1479310200,
        "origin": "91b3ec58a632b5d03",
        "id": "0b08394cd20756dab",
        "duration": 50,
        "readonly": false,
        "request_id": "0b08394cd20756dab",
        "talk": {
          "description": "Apache Spark is one of the most popular tools for big data, and with over 400 open pull requests as of this writing very active in terms of development as well. With such a large volume of contributions, it can feel difficult to started contributing to Apache Spark. This talk is developer focused and will walk through how to find good issues to start with, formatting code, finding reviewers, and what to expect in the code review process. We will also talk about alternatives to contributing to Apache Spark directly (such as creating packages).",
          "onhold": false,
          "bio": "Holden Karau is a software development engineer and is active in open source. She a co-author of Learning Spark & Fast Data Processing with Spark and has taught intro Spark workshops. Prior to IBM she worked on a variety of big data, search, and classification problems at Alpine, DataBricks, Google, Foursquare, and Amazon. She graduated from the University of Waterloo with a Bachelors of Mathematics in Computer Science. Outside of computers she enjoys dancing & playing with fire.",
          "request_id": "6d07257d822322c96764d13",
          "category": "Spark",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.5596,
          "notes": "",
          "tags": "",
          "id": "6d07257d822322c96764d13",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Getting Started Contributing to Apache Spark",
          "speaker": "Holden Karau",
          "accepted": true,
          "submitter": "6004e81fff1f0ffdc3cebab3c0516e7bad0e86b420aebd0fe10bb4c3"
        },
        "room": "91b3ec58a632b5d03",
        "day": 1479254400,
        "assignee": "6d07257d822322c96764d13",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479313200
      }, {
        "starttime": 1479313800,
        "origin": "91b3ec58a632b5d03",
        "id": "821a9fbaa7387fc81",
        "duration": 50,
        "readonly": false,
        "request_id": "821a9fbaa7387fc81",
        "talk": {
          "description": "Big Data is all about being to access and process data in various formats, and from various sources. Apache Bahir provides extensions to distributed analytic platforms providing them access to different data sources. In this talk we will introduce you to Apache Bahir and its various connectors that are available for Apache Spark and Apache Flink. We will also go over the details of how to build, test and deploy an Spark Application using the MQTT data source for the new Apache Spark 2.0 Structure Streaming functionality.",
          "onhold": false,
          "bio": "Luciano Resende is an Architect in IBM Analytics. He has been contributing to open source at The ASF for over 10 years, he is a member of ASF and is currently contributing to various big data related Apache projects including Spark, Zeppelin, Bahir. Luciano is the project chair for Apache Bahir, and also spend time as mentor newly created  Apache Incubator projects. At IBM, he contributed to several IBM big data offerings, including BigInsights, IOP and its respective Bluemix Cloud services.",
          "request_id": "758c920564a2c12b761963e",
          "category": "Spark",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.8186,
          "notes": "",
          "tags": "",
          "id": "758c920564a2c12b761963e",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Writting Apache Spark and Apache Flink applications using Apache Bahir",
          "speaker": "Luciano Resende",
          "accepted": true,
          "submitter": "e75be01b191373cd5af79fbbe3c5bec34983c210ba6e9a27e8e8a207"
        },
        "room": "91b3ec58a632b5d03",
        "day": 1479254400,
        "assignee": "758c920564a2c12b761963e",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479316800
      }]
    }]
  }, {
    "id": "1fe45465498f99a4f",
    "name": "Giralda 6\/7",
    "days": [{
      "day": 1479340800,
      "slots": [{
        "starttime": 1479380400,
        "origin": "1fe45465498f99a4f",
        "id": "d1f73e7623699a209",
        "duration": 50,
        "readonly": false,
        "request_id": "d1f73e7623699a209",
        "talk": {
          "description": "Apache Commons Crypto is a cross-platform cryptographic library optimized with hardware acceleration. It provides Java API for both cipher level and stream level, developers can use it to implement strong and high performance AES encryption\/decryption with the minimum code and effort. This presentation will introduce some typical examples of data encryption on big data area, how does CRYPTO accelerating data encryption, especially in Apache Hadoop, Apache Spark, and the future work of Apache Commons Crypto.",
          "onhold": false,
          "bio": "Dapeng Sun is a software engineer at Intel and focuses on big data security and optimization. He is the PMC of Apache Sentry, release manager and committer of Apache Commons. He also contributed to Apache Hadoop, Apache Hive, Alluxio (Formerly Tachyon) and involved in the development of Intel Distribution Hadoop, Project-rhino, Hadoop Token based authentication, and etc. ",
          "request_id": "d5ee704942fb914e501161e",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.2268,
          "notes": "",
          "tags": "",
          "id": "d5ee704942fb914e501161e",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Apache Commons Crypto: Accelerating big data encryption",
          "speaker": "Dapeng Sun",
          "accepted": true,
          "submitter": "7d19d4eea53adc67e0a53c18d218261139fe9b035b8ca89e853100ff"
        },
        "room": "1fe45465498f99a4f",
        "day": 1479340800,
        "assignee": "d5ee704942fb914e501161e",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479383400
      }, {
        "starttime": 1479384000,
        "origin": "1fe45465498f99a4f",
        "id": "efe0661885e5663b6",
        "duration": 50,
        "readonly": false,
        "request_id": "efe0661885e5663b6",
        "talk": {
          "description": "The hash function is the veritable hammer for pounding a large array of engineering problems into submission. Want to shard your database? Draw a key from your data, hash it, and voila, instant deterministic load balancing! Thatäó»s simple enough, until you look more carefully at distributional effects, failure, and redundancy management. Weäó»ll review well known (consistent hashing), not so well known (rendezvous hashing), and recent (shuffle sharding, copysets) work that goes a long way towards engineering more favorable failure scenarios.\n\n\n\nThis talk not only covers old and new techniques, but also digs a little into the math, showcases an open source tool, and demonstrates how we might apply these techniques to other infrastructure such as load balancing.",
          "onhold": false,
          "bio": "Wes has a B.S. in Electrical Engineering & Computer Science from UC Berkeley. He spent eight years building technical infrastructure for high frequency trading shops. One day, he stared into his dark soul and realized he needed to move into the startup light. Thus S7 Labs sprang into being, and he led teams that built Storybox, a Seedcamp NY finalist, and Songza Radio, subsumed by Google Music. He's now at Chartbeat serving out his term as CTO.\n\n",
          "request_id": "0d5efbd3b8522c3f0ad6ceb",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.2074,
          "notes": "",
          "tags": "",
          "id": "0d5efbd3b8522c3f0ad6ceb",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Consistent Hashing, Shuffle Sharding, and Copysets: Practical Tools For Controlling Failure",
          "speaker": "Wes Chow",
          "accepted": true,
          "submitter": "ad9bf2650d3890a57532e91f68dd51bb5b315b1be697dc6d7490eff2"
        },
        "room": "1fe45465498f99a4f",
        "day": 1479340800,
        "assignee": "0d5efbd3b8522c3f0ad6ceb",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479387000
      }, {
        "starttime": 1479387600,
        "origin": "1fe45465498f99a4f",
        "id": "55dba07b1fe390457",
        "duration": 50,
        "readonly": false,
        "request_id": "55dba07b1fe390457",
        "talk": {
          "description": "Classification algorithms play an important role in different business areas, such as fraud detection, cross selling or customer behavior. In the business context, interpretability is a very desirable property, sometimes even a hard requirement. However, interpretable algorithms are usually outperformed by other non-interpretable algorithms such as Random Forest. In this talk Antonio Soriano will present a distributed implementation in Spark of the Logistic Model Tree (LMT) algorithm (Landwehr, et al. (2005). Machine Learning, 59(1-2), 161-205.), which consists of a decision tree with logistic classifiers in the leafs. While being highly interpretable, the LMT consistently performs equal or better than other popular algorithms in several performance metrics such as accuracy, precision\/recall or area under the ROC curve.",
          "onhold": false,
          "bio": "Mateo ílvarez studied aerospace engineering at the Universidad Polití©cnica de Madrid, with a masters degree in Propulsion Systems, and Data Science in the Universidad Rey Juan Carlos. He is passionate about data analysis with Scala, Python and all Big Data technologies, and is currently part of the Data Science team at Stratio Big Data, working with ML algorithms, profiling analysis based around Spark.\n\n\n\nWe at Stratio have been working with Big Data for over a decade. We foresaw that the future of big data was fast data, & started working with Apache Spark before it was in incubation. We brought the 1st platform to market in early 2014, currently in production in over a dozen large enterprises.",
          "request_id": "c7b2b83980546cff546de6e",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.6759,
          "notes": "",
          "tags": "",
          "id": "c7b2b83980546cff546de6e",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Distributed Logistic Model Trees",
          "speaker": "Mateo Alvarez",
          "accepted": true,
          "submitter": "67b9c9efa31a08735572ab84817059b40651751f71749cb146156605"
        },
        "room": "1fe45465498f99a4f",
        "day": 1479340800,
        "assignee": "c7b2b83980546cff546de6e",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479390600
      }]
    }, {
      "day": 1479168000,
      "slots": [{
        "starttime": 1479207600,
        "origin": "1fe45465498f99a4f",
        "id": "9bf196d3de6db5eb3",
        "duration": 50,
        "readonly": false,
        "request_id": "9bf196d3de6db5eb3",
        "talk": {
          "description": "This talk will help you build data mining and machine learning applications using Apache Giraph framework for graph processing. This talk is based on the \"Practical Graph Analytics with Apache Giraph\" book trying to be as hands-on as possible. Apache Giraph offers a simple yet flexible programming model targeted to graph algorithms and designed to scale easily to accommodate massive amounts of data. Originally developed at Yahoo!, Giraph now enjoys a diverse community of contributors from who's-who of Silicon Valley companies: Facebook, LinkedIN and Twitter.",
          "onhold": false,
          "bio": "Roman Shaposhnik is a Director of Open Source at Pivotal Inc. He is a committer on Apache Hadoop, co-creator of Apache Bigtop and contributor to various other Hadoop ecosystem projects. He is also an ASF member and a former Chair of Apache Incubator. In his copious free time he managed to co-author \"Practical Graph Analytics with Apache Giraph\" and he also posts to twitter as @rhatr. Roman has been involved in Open Source software for more than a decade and hacked projects ranging from Linux kernel to FFmpeg multimedia library. He loves good beer and fights against American led IPA invasion one seidla at a time.",
          "request_id": "0ea3b1a95fb2358c592c75b",
          "category": "Graph",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.9842,
          "notes": "",
          "tags": "",
          "id": "0ea3b1a95fb2358c592c75b",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Practical Graph Analytics with Apache Giraph",
          "speaker": "Roman Shaposhnik",
          "accepted": true,
          "submitter": "56160bf05f664f18e00b6400cb8030f1a21abba3db18457146f8fca0"
        },
        "room": "1fe45465498f99a4f",
        "day": 1479168000,
        "assignee": "0ea3b1a95fb2358c592c75b",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479210600
      }, {
        "starttime": 1479211200,
        "origin": "1fe45465498f99a4f",
        "id": "2e42c60cfa1f315f0",
        "duration": 50,
        "readonly": false,
        "request_id": "2e42c60cfa1f315f0",
        "talk": {
          "description": "Since the last conference, Apache S2Graph community has been working on the integration with Apache Tinkerpop. Tinkerpop users are now able to use S2Graph as graph database without changing their Thinkerpop code, and also execute OLAP graph queries over their data in HDFS. We will share our experiences to integrate Thinkerpop as a graph database API, and comment on our current limitations and future plans. We will also present the benchmark results showing the comparison between S2Graph and existing graph databases such as Neo4j, Titan, and OrientDB. We focus our benchmarks on the \"neighbors of neighbors\" queries and the basic CRUD operations. Similar to Titan, S2Graph supports multiple storage backends, such as HBase, Cassandra, Mysql, Postgresql, and RocksDB, and the S2Graph's performance for each backend will be presented.",
          "onhold": false,
          "bio": "Doyung works in a distributed graph database team at Kakao as software engineer, where his focus is on performance and usability. He developed Apache S2Graph, an open-source distributed graph database, and has previously presented it at ApacheCon BigData Europe and ApacheCon BigData North America.",
          "request_id": "6d4572276b1d1ffeafd99f6",
          "category": "Graph",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.6884,
          "notes": "",
          "tags": "",
          "id": "6d4572276b1d1ffeafd99f6",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Graph processing with Apache Tinkerpop on Apache S2Graph",
          "speaker": "Doyung Yoon",
          "accepted": true,
          "submitter": "aeb7741846a150bf6aa3ea2d0d2b741a2415f70508bd39bd0d886949"
        },
        "room": "1fe45465498f99a4f",
        "day": 1479168000,
        "assignee": "6d4572276b1d1ffeafd99f6",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479214200
      }, {
        "starttime": 1479214800,
        "origin": "1fe45465498f99a4f",
        "id": "f3a7f754e7c9e4865",
        "duration": 50,
        "readonly": false,
        "request_id": "f3a7f754e7c9e4865",
        "talk": {
          "description": "S2Graph is a graph database designed to handle transactional graph processing at scale.\n\nIts API allows you to store, manage and query relational information using edge and vertex representations in a fully asynchronous and non-blocking manner.\n\nHowever, at Kakao Corp., where the project was originally started, we believe that it could be so much more.\n\nThere have been efforts to utilize S2Graph as the centerpiece of Kakaoäó»s event delivery system taking advantage of its strengths such as\n\n- flexibility of seamless bulk loading, AB testing, and stored procedure features,\n\n- multitenancy that allows interoperability among different services within the company,\n\n- and most of all, the ability to run various operations ranging from basic CRUD to multi-step graph traversal queries in realtime with large volumes.",
          "onhold": false,
          "bio": "Seoul-based developer interested in large scale data systems and cloud computing.\n\nCurrently, working as a data systems developer at Kakao Corp., Korea with open source projects such as Apache S2Graph (incubating) and Druid among others.\n\nPrevious work experience include software development at Siemens AG, Germany and Samsung Electronics, Korea.\n\nPresented a Lightning Talk at Apache: Big Data North America 2016, Vancouver, BC.",
          "request_id": "e00f85441112888f7a83326",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.7514,
          "notes": "",
          "tags": "",
          "id": "e00f85441112888f7a83326",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Apache S2Graph (incubating) as a User Event Hub",
          "speaker": "Hyunsung Jo",
          "accepted": true,
          "submitter": "ee5a6498c09f4e0eb4dbad5ff81394b4ff1fcb759b6917ade2d89f09"
        },
        "room": "1fe45465498f99a4f",
        "day": 1479168000,
        "assignee": "e00f85441112888f7a83326",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479217800
      }, {
        "starttime": 1479223800,
        "origin": "1fe45465498f99a4f",
        "id": "5ce5337377f40feab",
        "duration": 50,
        "readonly": false,
        "request_id": "5ce5337377f40feab",
        "talk": {
          "description": "In the movies, gremlins spawn quickly in water and become hard to pin down. Now, a similar situation has happened at the Apache Software Foundation. Apache TinkerPop graduated to a top level project, and it continues to expand the graph computing space. Come learn the tale of how Gremlin, the graph query language, is evolving to reach new developers across the world, no matter what programming language they use. We will take a close look at how Gremlin language drivers and Gremlin language variants are making for a consistent graph experience powered by Apache TinkerPop. We will look specifically at Gremlin-Python variants, which enable the notebook-style development favored by data scientists for interactive data analytics.",
          "onhold": false,
          "bio": "Jason Plurad is a software engineer from IBM Open Technology. He is a PMC member and committer on Apache TinkerPop, an open source graph computing framework. Jason engages in various development (including front end, web tier, NoSQL databases, and big data analytics) and promotes adoption of open source technologies into enterprise applications, service, and solutions. He has spoken previously at Apache: Big Data, IBM conferences (Innovate, Insight, World of Watson), Scylla Summit, and Triangle Hadoop Users Group meetups.",
          "request_id": "7d426f8f7d5a473ebbac66f",
          "category": "graph",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.4427,
          "notes": "",
          "tags": "",
          "id": "7d426f8f7d5a473ebbac66f",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "The Wide World of Gremlin",
          "speaker": "Jason Plurad",
          "accepted": true,
          "submitter": "ce62161b12d9e148179fbcf9d64ece154933d7a3dcb69866946665ee"
        },
        "room": "1fe45465498f99a4f",
        "day": 1479168000,
        "assignee": "7d426f8f7d5a473ebbac66f",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479226800
      }, {
        "starttime": 1479227400,
        "origin": "1fe45465498f99a4f",
        "id": "d4ae299e6d14287df",
        "duration": 50,
        "readonly": false,
        "request_id": "d4ae299e6d14287df",
        "room": "1fe45465498f99a4f",
        "day": 1479168000,
        "assignee": null,
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479230400
      }]
    }, {
      "day": 1479254400,
      "slots": [{
        "starttime": 1479294000,
        "origin": "1fe45465498f99a4f",
        "id": "f12e58d357e0f527e",
        "duration": 50,
        "readonly": false,
        "request_id": "f12e58d357e0f527e",
        "talk": {
          "description": "Criteo had an Hadoop cluster with 39 PB raw stockage, 13404 CPUs, 105 TB RAM, 40 TB data imported per day and >100000 jobs per day. This cluster was critical in both stockage and compute but without backups. This talk describes: 0\/ the different options considered when deciding how to protect our data and compute capacity 1\/ the criteria established for the 800 new computers and comparison tests between suppliers' hardware 2\/ the non-blocking network infrastructure with 10 Gb\/s endpoints scalable to 5000 machines 3\/ the installation and configuration, using Chef, of a cluster on new hardware 4\/ the problems encountered in moving our jobs and data from the old CDH4 cluster to the new CDH5 cluster 600 km distant 5\/ running and feeding with data the two clusters in parallel 6\/ fail over plans 7\/ operational issues 8\/ the performance of the 16800 core, 200 TB RAM and 60 PB disk CDH5 cluster.",
          "onhold": false,
          "bio": "Stuart loves storage (100 PB at Criteo) and is part of Criteo's Lake team that runs some small and two huge Hadoop clusters. He also loves automation with Chef because configuring 2000 Hadoop nodes by hand is just too slow. He has spoken at Devoxx2016 and at Criteo Lab's NABD Conference 2016. Before discovering Hadoop he developed user interfaces for biotech companies.",
          "request_id": "43c707bd7e927d0cdf15386",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.3382,
          "notes": "",
          "tags": "",
          "id": "43c707bd7e927d0cdf15386",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Create a Hadoop cluster and migrate 39PB data plus 150000 jobs\/day",
          "speaker": "Stuart Pook",
          "accepted": true,
          "submitter": "7b00485c01fa0cb0426184e682a67a3a1c7a4f126ad28786158e5f91"
        },
        "room": "1fe45465498f99a4f",
        "day": 1479254400,
        "assignee": "43c707bd7e927d0cdf15386",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479297000
      }, {
        "starttime": 1479297600,
        "origin": "1fe45465498f99a4f",
        "id": "8d6de12081a8294d2",
        "duration": 50,
        "readonly": false,
        "request_id": "8d6de12081a8294d2",
        "talk": {
          "description": "How do you solve the hardest and most interesting data science problems out there? What if the usual methods and tricks donäó»t work? \n\n\n\nWhat do you do when there is no right answer to your problem? What do you do when you have some signal or event in your data that you want to detect thatäó»s so rare that generating enough labeled data to build a supervised model is impossible?  What if you donäó»t even know where to start when figuring out what variables are meaningful?\n\n\n\nIn this talk, Ilya presents a recent use case from Capital One that demands a fundamentally different data science than what one usually sees. Weäó»ll learn about bootstrapping a data science problem with limited data and sparse events and what techniques apply when youäó»re dealing with the vague and unpredictable. Join him for a dive into the exciting world of unsupervised learning and journey into the unknown!\n\n",
          "onhold": false,
          "bio": "Ilya is a roboticist turned data engineer. At the University of Michigan he built self-discovering robots and then worked on embedded DSP software with cell phone radios at Boeing. Today, he drives innovation at Capital One. Ilya is a contributor to the core components of Apache Spark and a PMC of Apache Apex with the goal of learning what it takes to build a next-generation distributed computing platform. He has presented at the Spark Summit and numerous tech meetups around the country.  ",
          "request_id": "ac5e5eb072dc47bd0568b26",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.1247,
          "notes": "",
          "tags": "",
          "id": "ac5e5eb072dc47bd0568b26",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "From Entropy to Eureka! - Find the needle in the needle",
          "speaker": "Ilya Ganelin",
          "accepted": true,
          "submitter": "e49a9ea6438c9fa564cebc6135cc4a446490fb38a7ce2233f968dbb2"
        },
        "room": "1fe45465498f99a4f",
        "day": 1479254400,
        "assignee": "ac5e5eb072dc47bd0568b26",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479300600
      }, {
        "starttime": 1479301200,
        "origin": "1fe45465498f99a4f",
        "id": "3b55f641bd3e422fe",
        "duration": 50,
        "readonly": false,
        "request_id": "3b55f641bd3e422fe",
        "talk": {
          "description": "trivago is processing roughly 7 billion events per day with an architecture that is entirely open source - from producing the data until its visualization in dashboards and reports. This talk will explain the idea behind the pipeline, highlight a particular business use case and share the experience and engineering challenges from two years in production. Clemens Valiente will furthermore show the different tools, frameworks and systems used, with Kafka for data ingestion, hadoop and Hive for processing and Impala for querying as the main focus. The successful implementation of this large scale data processing pipeline fundamentally transformed the way trivago was able to approach its business.",
          "onhold": false,
          "bio": "I'm leading the BI Data Engineering team at trivago where we are running a data processing pipeline through kafka, hadoop, impala and R processing roughly 7 billion events per day.\n\nI already presented this architecture at a very well-received meetup talk in Dí_sseldorf earlier this year, see: http:\/\/www.meetup.com\/Big-Data-Hadoop-Spark-NRW\/events\/229580043\/",
          "request_id": "421256f1d83b9f238760d39",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.5196,
          "notes": "",
          "tags": "",
          "id": "421256f1d83b9f238760d39",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Large scale open source data processing pipelines at trivago",
          "speaker": "Clemens Valiente",
          "accepted": true,
          "submitter": "b756b72e2be02dc0365ea4fe9b3057bda55ba9f8629d19a8ef52576d"
        },
        "room": "1fe45465498f99a4f",
        "day": 1479254400,
        "assignee": "421256f1d83b9f238760d39",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479304200
      }, {
        "starttime": 1479310200,
        "origin": "1fe45465498f99a4f",
        "id": "970b8c9a42bc1dcb2",
        "duration": 50,
        "readonly": false,
        "request_id": "970b8c9a42bc1dcb2",
        "talk": {
          "description": "Running a managed Solr service brings fun challenges with it, to both the users and the service itself. Users typically do not have access to all components of the Solr system (e.g. the ZK ensemble, the actual nodes that Solr runs on etc.). On the other hand the service must ensure high-availability at all times, and handle what is often user-driven tasks such as version upgrades, taking nodes offline for maintenance and more.\n\n\n\nIn this talk I will describe how we tackle these challenges to build a managed Solr service on the cloud, which currently hosts few thousands of Solr clusters. I will focus on the infrastructure that we chose to run the Solr clusters on, as well how we ensure high-availability, cluster balancing and version upgrades.",
          "onhold": false,
          "bio": "Shai Erera is a Researcher at IBM Research, Haifa, Israel. Shai earned his M.Sc in Computer Science from the University of Haifa in 2007. Shaiäó»s work experience includes the development of search-based systems over Lucene and Solr and he is also a Lucene\/Solr committer.",
          "request_id": "0ef9efb78d149cf98eccd81",
          "category": "solr",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.4623,
          "notes": "",
          "tags": "",
          "id": "0ef9efb78d149cf98eccd81",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Building and running a Solr-as-a-Service for IBM Watson",
          "speaker": "Shai Erera",
          "accepted": true,
          "submitter": "f8c5adc78765abee5ad3098e12991c00447cdc40cd3ad0ced69c2af5"
        },
        "room": "1fe45465498f99a4f",
        "day": 1479254400,
        "assignee": "0ef9efb78d149cf98eccd81",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479313200
      }, {
        "starttime": 1479313800,
        "origin": "1fe45465498f99a4f",
        "id": "9b04c684e39bbd9ac",
        "duration": 50,
        "readonly": false,
        "request_id": "9b04c684e39bbd9ac",
        "talk": {
          "description": "Which Big Data Platform shown I use for my problem? This question remains one of the most important question for practitioners. In this talk, we will present the universal benchmarking platform for Big Data HOBBIT (htpp:\/\/project-hobbit.eu). The platform providies a unified approach for benchmarking Big Data frameworks. Mimicking algorithms generated from real data ensure that the dataset used for benchmarking resemble real data but are open for all to use, therewith circumventing the issues that come about when using company-bound data. The core of the platform implements industry-relevant KPI gathered from more than 70 Big-Datad-driven organizations. The results are generated using machine-readable formats so as to ensure that they can be analyzed and use for improving toold and frameworks. In the talk, I will present the architecture of the framework and some preliminary results. ",
          "onhold": false,
          "bio": "Head of AKSW (http:\/\/aksw.org) at University of Leipzig\/InfAI, a research group with ca. 50 members. Author of 120+ research papers and 20+ presentations are top-tier conferences. Received manifold research awards including Next Einstein Forum award 2016, 12 best research paper awards and competition wins. Coached by Lisa Shufro (ex-TED coach) in speaking. Currently head of the HOBBIT project (http:\/\/project-hobbit.eu), which focuses on benchmarking Big Linked Data.",
          "request_id": "7857d184b0e3da9cc72dd67",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.3878,
          "notes": "",
          "tags": "",
          "id": "7857d184b0e3da9cc72dd67",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Unified Benchmarking of Big Data Platforms",
          "speaker": "Axel-Cyrille Ngonga Ngomo",
          "accepted": true,
          "submitter": "b0608b05ca1434719f2c33a5d677b17404f68c56468606627402d0a1"
        },
        "room": "1fe45465498f99a4f",
        "day": 1479254400,
        "assignee": "7857d184b0e3da9cc72dd67",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479316800
      }]
    }]
  }, {
    "id": "7e52efe276d9a4001",
    "name": "Nervion\/Arenal2",
    "days": [{
      "day": 1479340800,
      "slots": [{
        "starttime": 1479380400,
        "origin": "7e52efe276d9a4001",
        "id": "44adecd202f77a967",
        "duration": 50,
        "readonly": false,
        "request_id": "44adecd202f77a967",
        "talk": {
          "description": "Apache Beam is a unified programming model designed to provide efficient and portable data processing pipelines. The same Beam pipelines work in batch and streaming, and on a variety of open source and private cloud big data processing backends including Apache Flink, Apache Spark, and Google Cloud Dataflow. This talk will introduce Apache Beam's programming model and mechanisms for efficient execution. The speakers will show how to build Beam pipelines, and demo how to use it to execute the same code across different runners.\n\n",
          "onhold": false,
          "bio": "JB is Apache Beam's champion and a member of the Beam PPMC. He is a long-tenured Apache Member, serving on as PMC\/committer for 20 projects that range from integration to big data. \n\nDan is a Beam PPMC member, committer, and a Google software engineer working on Apache Beam and the Google Cloud Dataflow runner for Beam.",
          "request_id": "426ef5435f85f838e06846b",
          "category": "beam",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.359,
          "notes": "",
          "tags": "",
          "id": "426ef5435f85f838e06846b",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Introduction to Apache Beam",
          "speaker": "Jean-Baptiste Onofrí©",
          "accepted": true,
          "submitter": "f144f28cd878d925a6d6bd85e88d5a0ea51475553bb233e63bc5b8b7"
        },
        "room": "7e52efe276d9a4001",
        "day": 1479340800,
        "assignee": "426ef5435f85f838e06846b",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479383400
      }, {
        "starttime": 1479384000,
        "origin": "7e52efe276d9a4001",
        "id": "7ebb983b179731e72",
        "duration": 50,
        "readonly": false,
        "request_id": "7ebb983b179731e72",
        "talk": {
          "description": "This session will detail the overarching goal and purpose of the Apache Beam project. It will explain how to use Apache Beam to implement data processing jobs. When you have an existing big data platform, the continuous evolution of that platform is important. If youäó»re currently using Apache Hadoop MapReduce jobs to process your data, you may want to migrate to Apache Spark in order to leverage new features and improve performance. You may also want to implement streaming data processing in addition to your existing batch processing capabilities. Alternatively, you may want to look for some easy integration patterns to use or upgrade to different technologies. For instance, if today you are using Apache Spark, you may want to consider using Apache Flink for some use cases or as part of a proof-of-concept (PoC) to weigh its benefits. ",
          "onhold": false,
          "bio": "Jean-Baptiste Onofrí© is an ASF member and committer of Apache Karaf, Apache ServiceMix, Apache ACE, Apache Camel, Apache Archiva, Apache Kalumet, Apache Falcon, Apache Syncope and mentor on Apache Sirona. Jean-Baptiste brings over ten years of development experience to Talend, where he works as a SOA\/Software Architect. Working at Talend allows him to increase his contribution to Apache projects. He has experience in enterprise system environments (AIX, Solaris, BSD, Linux) and network. ",
          "request_id": "9c402946986469eb80bc4f9",
          "category": "beam",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.217,
          "notes": "",
          "tags": "",
          "id": "9c402946986469eb80bc4f9",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Going Under the Hood with Apache Beam",
          "speaker": "Siobhan Lyons",
          "accepted": true,
          "submitter": "45a7f8cc5ef0e4d5a67d0248c4245f9a28cc05d0db1870b2494c4625"
        },
        "room": "7e52efe276d9a4001",
        "day": 1479340800,
        "assignee": "9c402946986469eb80bc4f9",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479387000
      }, {
        "starttime": 1479387600,
        "origin": "7e52efe276d9a4001",
        "id": "748d9d21f04c507fd",
        "duration": 50,
        "readonly": false,
        "request_id": "748d9d21f04c507fd",
        "talk": {
          "description": "Learn about Scio, a Scala DSL for Apache Beam. Beam introduces a simple, unified programming model for both batch and streaming data processing while Scio brings it much closer to the high level API many data engineers are familiar with. We will cover design and implementation of the framework, including features like type safe BigQuery and REPL. There will also a live coding demo.",
          "onhold": false,
          "bio": "Neville is a software engineer at Spotify who works mainly on data infrastructure and tools for machine learning and advanced analytics. In the past few years he has been driving the adoption of Scala and new data tools for music recommendation, including Scalding, Spark, Storm and Parquet. Before that he worked on search quality at Yahoo! and old school distributed systems like MPI.",
          "request_id": "64cf16b49e35e14f00d7417",
          "category": "beam",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.3911,
          "notes": "",
          "tags": "",
          "id": "64cf16b49e35e14f00d7417",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Scio, a Scala DSL for Apache Beam",
          "speaker": "Neville Li",
          "accepted": true,
          "submitter": "04a26945dcf06260ca35ddbdb6942cd00106b73480499a5bb07f0194"
        },
        "room": "7e52efe276d9a4001",
        "day": 1479340800,
        "assignee": "64cf16b49e35e14f00d7417",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479390600
      }]
    }, {
      "day": 1479254400,
      "slots": [{
        "starttime": 1479294000,
        "origin": "7e52efe276d9a4001",
        "id": "19ebbe9e4b0d1c716",
        "duration": 50,
        "readonly": false,
        "request_id": "19ebbe9e4b0d1c716",
        "talk": {
          "description": "A thorough introduction to CouchDB 2.0, the five-years-in-the-making final delivery of the larger CouchDB vision.\n\n\n\nApache CouchDB 2,0 finally puts the C back in C.O.U.C.D.B: Cluster of unreliable commodity hardware. With a production-proofed implementation of the Amazon Dynamo paper, CouchDB has now high-availability, multi-machine clustering as well scaling options built-in, making it ready for Big Data solutions that benefit from CouchDBäó»s unique multi-master replication.\n\n",
          "onhold": false,
          "bio": "Jan Lehnardt is the PMC Chair and VP of Apache CouchDB, co-creator of the Hoodie web app framework based on CouchDB as well as the founder and CEO of Neighbourhoodie Software. Heäó»s the longest standing contributor to Apache CouchDB.",
          "request_id": "ac1e83127a571fdc4fb007c",
          "category": "nosql",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.6988,
          "notes": "",
          "tags": "",
          "id": "ac1e83127a571fdc4fb007c",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Introducing Apache CouchDB 2.0",
          "speaker": "Jan Lehnardt",
          "accepted": true,
          "submitter": "91a2b983bce853bf05871eedf5d87a465031f58e746a37b7a350e27f"
        },
        "room": "7e52efe276d9a4001",
        "day": 1479254400,
        "assignee": "ac1e83127a571fdc4fb007c",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479297000
      }, {
        "starttime": 1479297600,
        "origin": "7e52efe276d9a4001",
        "id": "d032fe8ae51650a76",
        "duration": 50,
        "readonly": false,
        "request_id": "d032fe8ae51650a76",
        "talk": {
          "description": "When working with BigData & IoT systems we often feel the need for a Common Query Language. The system specific languages usually require longer adoption time and are harder to integrate within the existing stacks. \n\nTo fill this gap some NoSql vendors are building SQL access to their systems. Building SQL engine from scratch is a daunting job and frameworks like Apache Calcite can help you with the heavy lifting. Calcite allow you to integrate SQL parser, cost-based optimizer, and JDBC with your NoSql system.\n\nWe will walk through the process of building a SQL access layer for Apache Geode (In-Memory Data Grid). I will share my experience, pitfalls and technical consideration like balancing between the SQL\/RDBMS semantics and the design choices and limitations of the data system.\n\nHopefully this will enable you to add SQL capabilities to your prefered NoSQL data system.",
          "onhold": false,
          "bio": "Christian Tzolov, Pivotal technical architect, BigData and Hadoop specialist, contributing to various open source projects. In addition to being an Apache¬ Committer and Apache Crunch PMC Member, he has spent over a decade working with various Java and Spring projects and has led several enterprises on large scale artificial intelligence, data science, and Apache Hadoop¬ projects. \n\ntwitter: @christzolov\n\nblog: http:\/\/blog.tzolov.net",
          "request_id": "ba06d9b10e5cc5864b4aaca",
          "category": "nosql",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.9166,
          "notes": "",
          "tags": "",
          "id": "ba06d9b10e5cc5864b4aaca",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Using Apache Calcite for enabling SQL and JDBC access to Apache Geode and other NoSQL data systems",
          "speaker": "Christian Tzolov",
          "accepted": true,
          "submitter": "1e0a9142690f5740f7bd3e4869be41d8cf583e37058062bba720e62d"
        },
        "room": "7e52efe276d9a4001",
        "day": 1479254400,
        "assignee": "ba06d9b10e5cc5864b4aaca",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479300600
      }, {
        "starttime": 1479301200,
        "origin": "7e52efe276d9a4001",
        "id": "187b24fa4753342ed",
        "duration": 50,
        "readonly": false,
        "request_id": "187b24fa4753342ed",
        "talk": {
          "description": "Apache Cassandra is a scalable database with high availability features. But they come with severe limitations in term of querying capabilities.\n\n\n\nSince the introduction of SASI in Cassandra 3.4, the limitations belong to the pass. Now you can create indices on your columns as well as benefit from full text search capabilities with the introduction of the new `LIKE '%term%'` syntax.\n\n\n\nTo illustrate how SASI works, we'll use a database of 100 000 albums and artists. We'll also show how SASI can help to accelerate analytics scenarios with Apache Spark using SparkSQL predicate push-down.\n\n\n\nWe also highlight some use-cases where SASI is not a good fit and should be avoided (there is no magic, sorry)\n\n",
          "onhold": false,
          "bio": "DuyHai Doan is a Cassandra technical advocate. He spends his time between technical presentations\/meetups on Cassandra, coding on open source projects to support the community and helping all  companies using Cassandra to make their project successful. He also gets an interest in all the eco-system around Cassandra (Spark, Zeppelin, ...). Previously he was working as a freelance Java\/Cassandra consultant",
          "request_id": "6ddb97d4561b1c0a7e5e3e8",
          "category": "nosql",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.3772,
          "notes": "",
          "tags": "",
          "id": "6ddb97d4561b1c0a7e5e3e8",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "SASI, Cassandra on the full text search ride !",
          "speaker": "DuyHai DOAN",
          "accepted": true,
          "submitter": "e2d91ebc85142b43539ae01682dcdb5352706e81c3d4fd761a95a369"
        },
        "room": "7e52efe276d9a4001",
        "day": 1479254400,
        "assignee": "6ddb97d4561b1c0a7e5e3e8",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479304200
      }, {
        "starttime": 1479310200,
        "origin": "7e52efe276d9a4001",
        "id": "822e86619b581bc11",
        "duration": 50,
        "readonly": false,
        "request_id": "822e86619b581bc11",
        "talk": {
          "description": "Cassandra is evolving at a very fast pace and keeps introducing new features that close the gap with traditional SQL world, but they are always designed with a distributed approach in mind.  \n\n\n\nFirst we'll throw an eye at the recent user-defined functions and show how they can improve your application performance and enrich your analytics use-cases.  \n\n\n\nNext, a tour on the materialized views, a major improvement that drastically changes the way people model data in Cassandra and makes developers' life easier!",
          "onhold": false,
          "bio": "DuyHai Doan is a Cassandra technical advocate. He spends his time between technical presentations\/meetups on Cassandra, coding on open source projects to support the community and helping all  companies using Cassandra to make their project successful. He also gets an interest in all the eco-system around Cassandra (Spark, Zeppelin, ...). Previously he was working as a freelance Java\/Cassandra consultant",
          "request_id": "e892c52cba737879e9e102c",
          "category": "nosql",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.3709,
          "notes": "",
          "tags": "",
          "id": "e892c52cba737879e9e102c",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "User Defined Functions and Materialized Views in Cassandra 3.0",
          "speaker": "DuyHai DOAN",
          "accepted": true,
          "submitter": "e2d91ebc85142b43539ae01682dcdb5352706e81c3d4fd761a95a369"
        },
        "room": "7e52efe276d9a4001",
        "day": 1479254400,
        "assignee": "e892c52cba737879e9e102c",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479313200
      }, {
        "starttime": 1479313800,
        "origin": "7e52efe276d9a4001",
        "id": "c2dbb1869db03113f",
        "duration": 50,
        "readonly": false,
        "request_id": "c2dbb1869db03113f",
        "room": "7e52efe276d9a4001",
        "day": 1479254400,
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479316800
      }]
    }, {
      "day": 1479168000,
      "slots": [{
        "starttime": 1479207600,
        "origin": "7e52efe276d9a4001",
        "id": "dafea981bb8f9c09f",
        "duration": 50,
        "readonly": false,
        "request_id": "dafea981bb8f9c09f",
        "talk": {
          "description": "Apache Hive has been continuously evolving to support a broad range of use cases, bringing it beyond its batch processing roots to its current support for interactive queries with LLAP. However, the development of its execution internals is not sufficient to guarantee efficient performance, since poorly optimized queries can create a bottleneck in the system. Hence, each release of Hive has included new features for its optimizer aimed to generate better plans and deliver improvements to query execution. In this talk, we present the development of the optimizer since its initial release. We describe its current state and how Hive leverages the latest Apache Calcite features to generate the most efficient execution plans. We show numbers demonstrating the improvements brought to Hive performance, and we discuss future directions for the next-generation Hive optimizer.",
          "onhold": false,
          "bio": "Jesí_s Camacho Rodrí_guez is a Member of Technical Staff at Hortonworks, actively contributing to Apache Hive and Apache Calcite. His work focuses on extending and improving query processing and optimization, ensuring that the increasingly complex workloads supported by Hive are executed quickly and efficiently. Prior to that, Jesí_s obtained his PhD in Computer Science from Universití© Paris-Sud and Inria, working on efficient techniques for large-scale Web data management.",
          "request_id": "6d1ac2e4104b3ebc509b566",
          "category": "hive",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.6815,
          "notes": "",
          "tags": "",
          "id": "6d1ac2e4104b3ebc509b566",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "An overview on optimization in Apache Hive: Past, Present, Future",
          "speaker": "Jesus Camacho Rodriguez",
          "accepted": true,
          "submitter": "4e81aa8a818171060e5d24253cb18d4ce2f50a2f0386c12caca93c0c"
        },
        "room": "7e52efe276d9a4001",
        "day": 1479168000,
        "assignee": "6d1ac2e4104b3ebc509b566",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479210600
      }, {
        "starttime": 1479211200,
        "origin": "7e52efe276d9a4001",
        "id": "bde26a593c939a54f",
        "duration": 50,
        "readonly": false,
        "request_id": "bde26a593c939a54f",
        "talk": {
          "description": "Apache Hive is the most commonly used SQL interface for Hadoop. To meet users data warehousing needs it must scale to petabytes of data, provide the necessary SQL, and perform in interactive time. The Hive community ihas produced a 2.0 release of Hive that includes significant improvements. These include:\n\n* LLAP, a daemon layer that enables sub-second response time.\n\n* HBase to store Hiveäó»s metadata, resulting in significantly reduced planning time.\n\n* Using Apache Calcite to build a cost based optimizer\n\n* Adding procedural SQL\n\n* Improvements in using Spark as an engine for Hive execution\n\nThis talk will cover the use cases these changes enable, the architectural changes being made in Hive as part of building these features, and share performance test results on how these improvements are speeding up Hive.",
          "onhold": false,
          "bio": "Alan is a founder of Hortonworks and an original member of the engineering team that took Pig from a Yahoo! Labs research project to a successful Apache open source project. Alan has done extensive work in Hive, including adding ACID transactions. Alan has a BS in Mathematics from Oregon State University and a MA in Theology from Fuller Theological Seminary. He is also the author of Programming Pig, a book from O'Reilly Press.",
          "request_id": "69f1321fce6f30011ea28b9",
          "category": "hive",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863395.2658,
          "notes": "",
          "tags": "",
          "id": "69f1321fce6f30011ea28b9",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Hive 2.0 SQL, Speed, Scale",
          "speaker": "Alan Gates",
          "accepted": true,
          "submitter": "b6fa66625a92657b218151442f491a6b93c0ec3e4e56d64ffece635c"
        },
        "room": "7e52efe276d9a4001",
        "day": 1479168000,
        "assignee": "69f1321fce6f30011ea28b9",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479214200
      }, {
        "starttime": 1479214800,
        "origin": "7e52efe276d9a4001",
        "id": "3441e0b451f454278",
        "duration": 50,
        "readonly": false,
        "request_id": "3441e0b451f454278",
        "talk": {
          "description": "Cloud deployments of Apache Hadoop are becoming more commonplace. Yet Hadoop and it's applications don't integrate that well äóîsomething which starts right down at the file IO operations.\n\n\n\nThis talk looks at how to make use of cloud object stores in Hadoop applications, including Hive and Spark. It will go from the \n\nfoundational \"what's an object store?\" to the practical \"what should I avoid\" and the timely \"what's new in Hadoop?\" äóî the latter covering the improved S3 support in Hadoop 2.8+.\n\n\n\nI'll explore the details of benchmarking and improving object store IO in Hive and Spark, showing what developers can do in order to gain performance improvements in their own code äóîand equally, what they must avoid.\n\n\n\nFinally, I'll look at ongoing work, especially \"S3Guard\" and what its fast and consistent file metadata operations promise.",
          "onhold": false,
          "bio": "Steve Loughran is a developer at Hortonworks, where he works on leading-edge Hadoop applications, most recently on Apache Slider and on Apache Spark's integration with Hadoop and YARN, and Hadoop's S3A connector to Amazon S3. He's the author of Ant in Action, a member of the Apache Software Foundation, and a committer on the Hadoop core since 2009. He lives and works in Bristol, England.",
          "request_id": "9141def958d9ef247c2d358",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.6125,
          "notes": "",
          "tags": "",
          "id": "9141def958d9ef247c2d358",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Hadoop, Hive, Spark and Object Stores",
          "speaker": "Steve Loughran",
          "accepted": true,
          "submitter": "747da6fd287fdaae865e110340ba8de17c3c0b114e9550e347ee0966"
        },
        "room": "7e52efe276d9a4001",
        "day": 1479168000,
        "assignee": "9141def958d9ef247c2d358",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479217800
      }, {
        "starttime": 1479223800,
        "origin": "7e52efe276d9a4001",
        "id": "8c85e14862028f4f5",
        "duration": 50,
        "readonly": false,
        "request_id": "8c85e14862028f4f5",
        "talk": {
          "description": "Understanding and analyzing Apache Hive query execution plan are crucial for performance debugging. In this talk, we highlight the difficulty of analyzing Apache Hiveäó»s query execution plan in the previous releases. We identify a set of pain points from our Apache Hive performance engineers, development engineers as well as real users. We propose and show a new presentation data model that can well address those pain points. The three most critical parts of the presentation are (1) the estimated and real query execution costs, which are the planner's guess at how long it will take to run the query before query is executed and the executor's record at how long it really takes to run the query; (2) the orchestration of the operator tree across consecutive vertices; and (3) integration and extension support with other presentation tools.",
          "onhold": false,
          "bio": "Pengcheng Xiong is a staff software engineer at Hortonworks. He is now working on the next-generation Hive optimizer and he serves and contributes as an Apache Hive PMC member and committer. He has extensive research and development experiences in centralized\/distributed RDBMS internals, Hive and etc. He has dozens of publications in database conferences, e.g., SIGMOD, VLDB and ICDE. He holds a PhD from Georgia Tech.",
          "request_id": "5bfc5b349f8a6a114bc1044",
          "category": null,
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.7721,
          "notes": "",
          "tags": "",
          "id": "5bfc5b349f8a6a114bc1044",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "How to understand and analyze Apache Hive query execution plan for performance debugging",
          "speaker": "pengcheng xiong",
          "accepted": true,
          "submitter": "f4913b5da0570e38e9ad1e77030b12112ff3e02be86ce3536119e224"
        },
        "room": "7e52efe276d9a4001",
        "day": 1479168000,
        "assignee": "5bfc5b349f8a6a114bc1044",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479226800
      }, {
        "starttime": 1479227400,
        "origin": "7e52efe276d9a4001",
        "id": "5f91e78045282b001",
        "duration": 50,
        "readonly": false,
        "request_id": "5f91e78045282b001",
        "talk": {
          "description": "Druid is an open-source analytics data store specially designed to execute OLAP queries on event data. Its speed, scalability and efficiency have made it a popular choice to power user-facing analytic applications. However, it does not provide important features requested by many of these applications, such as a SQL interface or support for complex operations such as joins. This talk presents our work on extending Druid indexing and querying capabilities using Apache Hive. In particular, our solution allows to index complex query results in Druid using Hive, query Druid data sources from Hive using SQL, and execute complex Hive queries on top of Druid data sources. We describe how we built an extension that brings benefits to both systems alike, leveraging Apache Calcite to overcome the challenge of transparently generating Druid JSON queries from the input Hive SQL queries.",
          "onhold": false,
          "bio": "Jesí_s Camacho Rodrí_guez is a Member of Technical Staff at Hortonworks, actively contributing to Apache Hive and Apache Calcite. His work focuses on extending and improving query processing and optimization, ensuring that the increasingly complex workloads supported by Hive are executed quickly and efficiently. Prior to that, Jesí_s obtained his PhD in Computer Science from Universití© Paris-Sud and Inria, working on efficient techniques for large-scale Web data management.",
          "request_id": "6b9e3d7785d46cd24ff0de4",
          "category": "hive",
          "conference": "apache-big-data-europe-2016",
          "level": 0,
          "created": 1473863394.9,
          "notes": "",
          "tags": "",
          "id": "6b9e3d7785d46cd24ff0de4",
          "pending": false,
          "ttype": "b6626d3814f174e0dd2e0d407",
          "title": "Interactive Analytics at Scale in Apache Hive using Druid",
          "speaker": "Jesus Camacho Rodriguez",
          "accepted": true,
          "submitter": "4e81aa8a818171060e5d24253cb18d4ce2f50a2f0386c12caca93c0c"
        },
        "room": "7e52efe276d9a4001",
        "day": 1479168000,
        "assignee": "6b9e3d7785d46cd24ff0de4",
        "title": "17:10 -> 18:00 (50 min)",
        "endtime": 1479230400
      }]
    }]
  }]
}